{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:25:02.409876Z",
     "iopub.status.busy": "2023-03-20T15:25:02.409460Z",
     "iopub.status.idle": "2023-03-20T15:26:09.637498Z",
     "shell.execute_reply": "2023-03-20T15:26:09.637430Z",
     "shell.execute_reply.started": "2023-03-20 15:25:02.414638+00:00"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-03-20 15:25:02,414.414 configure_magic] Magic cell payload received: {\"conf\": {\"spark.pyspark.python\": \"python3\", \"spark.pyspark.virtualenv.enabled\": \"true\", \"spark.pyspark.virtualenv.type\": \"native\", \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\", \"spark.sql.files.ignoreCorruptFiles\": \"true\", \"spark.dynamicAllocation.executorIdleTimeout\": \"18000\", \"spark.driver.memory\": \"12g\", \"spark.executor.memory\": \"12g\", \"spark.driver.cores\": \"3\", \"spark.driver.maxResultSize\": \"10g\", \"spark.yarn.executor.Overhead\": \"10g\", \"livy.server.session.timeout\": \"10h\"}, \"proxyUser\": \"assumed-role_fdp_blitvin-Isengard\"}\n",
      "\n",
      "[I 2023-03-20 15:25:02,414.414 configure_magic] Sending request to update kernel. Please wait while the kernel will be refreshed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel is successfully refreshed."
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "        \"spark.pyspark.python\": \"python3\"\n",
    "    ,\"spark.pyspark.virtualenv.enabled\": \"true\"\n",
    "    ,\"spark.pyspark.virtualenv.type\":\"native\"\n",
    "    ,\"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    ,\"spark.sql.files.ignoreCorruptFiles\":\"true\"\n",
    "    ,\"spark.dynamicAllocation.executorIdleTimeout\":\"18000\"\n",
    "    ,\"spark.driver.memory\":\"12g\",\"spark.executor.memory\":\"12g\"\n",
    "    ,\"spark.driver.cores\":\"3\"\n",
    "    ,\"spark.driver.maxResultSize\":\"10g\"\n",
    "    ,\"spark.yarn.executor.Overhead\":\"10g\"\n",
    "    ,\"livy.server.session.timeout\":\"10h\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/basics-of-apache-spark-configuration-settings-ca4faff40d45\n",
    "https://luminousmen.com/post/spark-tips-partition-tuning\n",
    "https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-partitionby/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:30:55.015888Z",
     "iopub.status.busy": "2023-03-20T15:30:55.015459Z",
     "iopub.status.idle": "2023-03-20T15:30:57.401808Z",
     "shell.execute_reply": "2023-03-20T15:30:57.401238Z",
     "shell.execute_reply.started": "2023-03-20T15:30:55.015800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp==3.8.1\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib64/python3.7/site-packages (from aiohttp==3.8.1) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib64/python3.7/site-packages (from aiohttp==3.8.1) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib64/python3.7/site-packages (from aiohttp==3.8.1) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (0.13.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp==3.8.1) (3.4)\n",
      "Installing collected packages: aiohttp\n",
      "Successfully installed aiohttp-3.8.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "awswrangler 2.19.0 requires pyarrow<10.1.0,>=2.0.0, but you'll have pyarrow 11.0.0 which is incompatible.\n",
      "aiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you'll have botocore 1.29.69 which is incompatible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.install_pypi_package(\"aiohttp==3.8.1\")\n",
    "except: \n",
    "    print(f'aiohttp is installed')\n",
    "try:\n",
    "    import pandas as pd\n",
    "except: \n",
    "    sc.install_pypi_package(\"pandas==1.1.5\")\n",
    "    import pandas as pd\n",
    "try:\n",
    "    import pyarrow\n",
    "except: \n",
    "    sc.install_pypi_package(\"pyarrow==0.14.1\")\n",
    "    import pyarrow \n",
    "try:\n",
    "    import s3fs\n",
    "except: \n",
    "    sc.install_pypi_package(\"s3fs\")\n",
    "    import s3fs \n",
    "try:\n",
    "    import fsspec\n",
    "except: \n",
    "    sc.install_pypi_package(\"fsspec\")\n",
    "    import fsspec \n",
    "if False:\n",
    "    try:\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "    except: \n",
    "        sc.install_pypi_package(\"matplotlib\")\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as py_f\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:30:57.403172Z",
     "iopub.status.busy": "2023-03-20T15:30:57.402886Z",
     "iopub.status.idle": "2023-03-20T15:30:57.457413Z",
     "shell.execute_reply": "2023-03-20T15:30:57.456860Z",
     "shell.execute_reply.started": "2023-03-20T15:30:57.403151Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0-amzn-1 \n",
      "\n",
      "\n",
      "('spark.kubernetes.executor.pod.allowlistFile', '/etc/spark/conf/executor-pod-template-pod-allowlist.txt')\n",
      "('spark.eventLog.enabled', 'true')\n",
      "('spark.pyspark.virtualenv.packages', 'aiohttp==3.8.1')\n",
      "('spark.eventLog.dir', 'file:///var/log/spark/apps')\n",
      "('spark.kubernetes.memoryOverheadFactor', '0.4')\n",
      "('spark.kubernetes.executor.podTemplateContainerName', 'spark-kubernetes-executor')\n",
      "('spark.app.id', 'spark-53fab8b52ed84ee5b8e16ffc516efe61')\n",
      "('spark.kubernetes.driverEnv.HTTP2_DISABLE', 'true')\n",
      "('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter')\n",
      "('spark.kubernetes.executor.selector.node.role', 'managed-endpoint-1-notebook')\n",
      "('spark.blacklist.decommissioning.timeout', '1h')\n",
      "('spark.kubernetes.driver.node.selector.node-lifecycle', 'on-demand')\n",
      "('spark.app.name', 'k24da05fc-f56d-4d6d-87d1-e71f96ef114a')\n",
      "('spark.hadoop.dynamodb.customAWSCredentialsProvider', 'com.amazonaws.auth.WebIdentityTokenCredentialsProvider')\n",
      "('spark.kubernetes.driver.container.allowlistFile', '/etc/spark/conf/driver-pod-template-container-allowlist.txt')\n",
      "('spark.driver.cores', '3')\n",
      "('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions')\n",
      "('spark.dynamicAllocation.executorAllocationRatio', '1')\n",
      "('spark.driver.maxResultSize', '10g')\n",
      "('spark.kubernetes.driver.podTemplateContainerName', 'spark-kubernetes-driver')\n",
      "('spark.history.fs.logDirectory', 'file:///var/log/spark/apps')\n",
      "('spark.kubernetes.executor.label.kernel_id', '24da05fc-f56d-4d6d-87d1-e71f96ef114a')\n",
      "('spark.kubernetes.executor.node.selector.spark-role', 'executor')\n",
      "('spark.yarn.heterogeneousExecutors.enabled', 'false')\n",
      "('spark.driver.extraLibraryPath', '/etc/hadoop/conf:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.pyspark.python', 'python3')\n",
      "('spark.yarn.executor.Overhead', '10g')\n",
      "('spark.kubernetes.container.image.pullPolicy', 'Always')\n",
      "('spark.kubernetes.submitInDriver', 'true')\n",
      "('spark.kubernetes.driver.node.selector.spark-role', 'driver')\n",
      "('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70\")\n",
      "('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.executor.extraClassPath', '/etc/hadoop/conf:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.kubernetes.executor.podNamePrefix', 'k24da05fc-f56d-4d6d-87d1-e71f96ef114a-2e4ee286ffa0c0f7')\n",
      "('spark.driver.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70\")\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.kubernetes.driver.podTemplateValidation.enabled', 'true')\n",
      "('spark.hadoop.hive.metastore.client.factory.class', 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory')\n",
      "('spark.kubernetes.allocation.batch.size', '5')\n",
      "('spark.driver.port', '7078')\n",
      "('spark.executorEnv.LOG_CONTEXT_WITH_PATH_SEPARATOR', '')\n",
      "('spark.decommissioning.timeout.threshold', '20')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true')\n",
      "('spark.kubernetes.driver.pod.allowlistFile', '/etc/spark/conf/driver-pod-template-pod-allowlist.txt')\n",
      "('spark.pyspark.virtualenv.enabled', 'true')\n",
      "('spark.kubernetes.driver.label.component', 'kernel')\n",
      "('spark.dynamicAllocation.shuffleTracking.timeout', '300s')\n",
      "('spark.dynamicAllocation.executorIdleTimeout', '18000')\n",
      "('spark.sql.files.ignoreCorruptFiles', 'true')\n",
      "('spark.kubernetes.executor.label.app', 'enterprise-gateway')\n",
      "('spark.pyspark.virtualenv.bin.path', '/usr/bin/virtualenv')\n",
      "('spark.kubernetes.driver.container.image', '614393260192.dkr.ecr.us-east-2.amazonaws.com/cdk-hnb659fds-container-assets-614393260192-us-east-2:754519b19b5f65eae94d49cadef904b39b505c739ff4f9aae8cafa83e51af8b9')\n",
      "('spark.kubernetes.client.dependency.propagation', 'false')\n",
      "('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000')\n",
      "('spark.authenticate', 'true')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2')\n",
      "('spark.dynamicAllocation.maxExecutors', '50')\n",
      "('spark.shuffle.service.enabled', 'false')\n",
      "('spark.kubernetes.executor.podTemplateFile', '/opt/spark/pod-template/pod-spec-template.yml')\n",
      "('spark.kubernetes.driver.request.cores', '0.5')\n",
      "('spark.driver.memory', '12g')\n",
      "('spark.kubernetes.driver.pod.name', 'k24da05fc-f56d-4d6d-87d1-e71f96ef114a-50f71686ff9fd0b0-driver')\n",
      "('spark.pyspark.virtualenv.type', 'native')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.executor.podTemplateValidation.enabled', 'true')\n",
      "('spark.hadoop.fs.defaultFS', 'file:///')\n",
      "('spark.app.startTime', '1679325969933')\n",
      "('spark.app.submitTime', '1679325968199')\n",
      "('spark.executor.cores', '4')\n",
      "('spark.driver.bindAddress', '10.0.96.172')\n",
      "('spark.kubernetes.executor.node.selector.node-lifecycle', 'spot')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.kubernetes.submission.waitAppCompletion', 'false')\n",
      "('spark.kubernetes.executor.label.emr-containers.amazonaws.com/kernel-type', 'PySpark')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.master', 'k8s://https://172.20.0.1:443')\n",
      "('spark.kubernetes.executor.container.allowlistFile', '/etc/spark/conf/executor-pod-template-container-allowlist.txt')\n",
      "('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true')\n",
      "('spark.dynamicAllocation.shuffleTracking.enabled', 'true')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true')\n",
      "('spark.kubernetes.driver.label.kernel_id', '24da05fc-f56d-4d6d-87d1-e71f96ef114a')\n",
      "('spark.kubernetes.namespace', 'adxuseast2emr')\n",
      "('spark.kubernetes.executor.label.component', 'worker')\n",
      "('spark.history.ui.port', '18080')\n",
      "('spark.driver.blockManager.port', '7079')\n",
      "('spark.hadoop.fs.s3.customAWSCredentialsProvider', 'com.amazonaws.auth.WebIdentityTokenCredentialsProvider')\n",
      "('spark.executor.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.driver.extraClassPath', '/etc/hadoop/conf:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.resourceManager.cleanupExpiredHost', 'true')\n",
      "('spark.driver.host', 'spark-8f682986ff9fd465-driver-svc.adxuseast2emr.svc')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'emr-containers-sa-spark-jeg-kernel-614393260192-1290uyahhbialm60icp0xrrxth1kcn9j6a76dmo6zat9i8skpkt4681h0l')\n",
      "('spark.kubernetes.driver.label.app', 'enterprise-gateway')\n",
      "('spark.files.fetchFailure.unRegisterOutputOnHost', 'true')\n",
      "('spark.kubernetes.driver.label.emr-containers.amazonaws.com/kernel-type', 'PySpark')\n",
      "('spark.executor.memory', '12g')\n",
      "('spark.kubernetes.executor.container.image', '614393260192.dkr.ecr.us-east-2.amazonaws.com/cdk-hnb659fds-container-assets-614393260192-us-east-2:754519b19b5f65eae94d49cadef904b39b505c739ff4f9aae8cafa83e51af8b9')\n",
      "('spark.executor.extraLibraryPath', '/etc/hadoop/conf:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.eventLog.logBlockUpdates.enabled', 'true')\n",
      "('spark.kubernetes.resource.type', 'python')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.kubernetes.driver.selector.node.role', 'managed-endpoint-1-notebook')\n",
      "('spark.dynamicAllocation.minExecutors', '0')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.kubernetes.authenticate.executor.serviceAccountName', 'emr-containers-sa-spark-jeg-kernel-614393260192-1290uyahhbialm60icp0xrrxth1kcn9j6a76dmo6zat9i8skpkt4681h0l')\n",
      "('spark.kubernetes.executor.request.cores', '3.5')\n",
      "('spark.blacklist.decommissioning.enabled', 'true')\n"
     ]
    }
   ],
   "source": [
    "print(spark.version,\"\\n\\n\")\n",
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for conf in configurations:\n",
    "    print(conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyzed the data published over the SIP feeds (CTA : CQS / CTS and UTP : UQDF / UTDF) and the depth of the book feeds of the 16 lit venues under the Reg. NMS system to see the impact of these proposed reforms on the quality of the market. \n",
    "\n",
    "Definitions \n",
    "Quoted spread = (bid - ask) / midpoint \n",
    "Spread -  bid - ask \n",
    "\n",
    "Results - \n",
    "1)\tNumber of current odd - lot trades within each bucket. & number of trades in each bucket. \n",
    "    a)\tGraphic concentration \n",
    "    b)\t– look for rationale & see if it is – \n",
    "2)\tAverage round-lot and odd-lot quoted spreads across each bucket \n",
    "    a)\tmatrix \n",
    "    b)\tHeat map \n",
    "    c)\tHour of the day ? \n",
    "3)\tEffect on market data-  \n",
    "    a)\tAnticipated increase in MD volumes - counts, etc\n",
    "    b)\tNumber of direct feed updates where top of the book is an odd - lot\n",
    "4)\tCase study around AMZN stock split - \n",
    "    a)\tRound lot spreads for AMZN per exchange - when high priced before the split\n",
    "    b)\tOdd-lot spreads for AMZN per exchange  - after the split. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:30:57.458738Z",
     "iopub.status.busy": "2023-03-20T15:30:57.458333Z",
     "iopub.status.idle": "2023-03-20T15:34:03.305674Z",
     "shell.execute_reply": "2023-03-20T15:34:03.305018Z",
     "shell.execute_reply.started": "2023-03-20T15:30:57.458719Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://maystreetdata/feeds_norm/partition_scheme_experiments_7/mstnorm_parquet_0_5_0\n",
      "mt_oddlot\n",
      "mt_roundlot_bbo\n",
      "mt_roundlot_nbbo\n",
      "mt_oddlot_prepped\n",
      "mt_roundlot_bbo_prepped\n",
      "mt_roundlot_nbbo_prepped\n"
     ]
    }
   ],
   "source": [
    "class MtRoundLot():\n",
    "    def __init__(self,part_experiment_id):\n",
    "        self.s3_dir_root=\"s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0\"\n",
    "        self.s3_dir_root_prepped=\"s3://maystreetdata/feeds_norm/partition_scheme_experiments_7/mstnorm_parquet_0_5_0/\"\n",
    "        self.s3_dir_partition_experiments =f\"s3://maystreetdata/feeds_norm/partition_scheme_experiments_{part_experiment_id}/mstnorm_parquet_0_5_0\"\n",
    "        self.s3_dir_root_results =f\"s3://maystreetdata/analysis/\"\n",
    "        self.tables={\"mt_roundlot_bbo\":{\"tables\":[f\"{self.s3_dir_root}/mt=bbo_quote/\"]},\n",
    "                     \"mt_roundlot_nbbo\":{\"tables\":[f\"{self.s3_dir_root}/mt=nbbo_quote/\"]},\n",
    "                     \"mt_oddlot\":{\"tables\":[f\"{self.s3_dir_root}/mt=aggregated_price_update/\"]}\n",
    "                    }\n",
    "        self.tables_prepped={\"mt_roundlot_bbo\":{\"tables\":[f\"{self.s3_dir_root_prepped}/mt_roundlot_bbo.parquet\"]},\n",
    "                     \"mt_roundlot_nbbo\":{\"tables\":[f\"{self.s3_dir_root_prepped}/mt_roundlot_nbbo.parquet\"]},\n",
    "                     \"mt_oddlot\":{\"tables\":[f\"{self.s3_dir_root_prepped}/mt_oddlot.parquet\"]}\n",
    "                    }\n",
    "        self.raw_df ={}\n",
    "        self.raw_df_prepped ={}\n",
    "        self.stats_df ={}\n",
    "        self.data_validation ={}\n",
    "        self.column_map={}\n",
    "        self.joined_df={}\n",
    "        self.column_map['mt_oddlot']={\n",
    "            'ask':'AskPrice_1'\n",
    "            ,'bid':'BidPrice_1'\n",
    "            ,'timestamp':'LastExchangeTimestamp'\n",
    "            ,'seq_number':'LastSequenceNumber'\n",
    "            ,'partition_by':['Product','FeedType',\"Feed\",\"dt\",'f','MarketParticipant','is_trading_hours','hour_est']\n",
    "        }\n",
    "        self.column_map['mt_roundlot_bbo']={\n",
    "            'ask':'AskPrice'\n",
    "            ,'bid':'BidPrice'\n",
    "            ,'timestamp':'ExchangeTimestamp'\n",
    "            ,'seq_number':'SequenceNumber'\n",
    "            ,'partition_by':['Product','FeedType',\"Feed\",\"dt\",'f','is_trading_hours','hour_est']\n",
    "        }\n",
    "        self.column_map['mt_roundlot_nbbo']={\n",
    "            'ask':'AskPrice'\n",
    "            ,'bid':'BidPrice'\n",
    "            ,'timestamp':'ExchangeTimestamp'\n",
    "            ,'seq_number':'SequenceNumber'\n",
    "            ,'partition_by':['Product','FeedType',\"Feed\",\"dt\",'f','is_trading_hours','hour_est']\n",
    "        }\n",
    "        self.round_factor=0.333333333333\n",
    "        print(self.s3_dir_partition_experiments)\n",
    "    def set_data_prepped(self,data_label):\n",
    "        col_map=self.column_map.get(data_label)\n",
    "        data_files = self.tables_prepped.get(data_label).get('tables')\n",
    "        data_df=None\n",
    "        for one_file in data_files:\n",
    "            one_data_df=spark.read.parquet(one_file)\n",
    "            if data_df is None:\n",
    "                data_df=one_data_df\n",
    "            else:\n",
    "                data_df=data_df.union(one_data_df)\n",
    "        self.raw_df_prepped[f\"{data_label}\"]=data_df\n",
    "    def set_data(self,data_label, is_raw=False):\n",
    "        col_map=self.column_map.get(data_label)\n",
    "        data_files = self.tables.get(data_label).get('tables')\n",
    "        data_df=None\n",
    "        for one_file in data_files:\n",
    "            feed_filters = self.tables.get(data_label).get('feeds',None)\n",
    "            path_parts= one_file.split(\"/\")\n",
    "            feed_type=path_parts[len(path_parts)-2:len(path_parts)-1][0]\n",
    "            if feed_filters is not None:\n",
    "                filter_string='\"'+'\",\"'.join(feed_filters)+'\"'\n",
    "                one_data_df = spark.read.parquet(one_file).filter(f'Feed in ({filter_string})') \n",
    "            else:\n",
    "                one_data_df = spark.read.parquet(one_file)\n",
    "            if 'f' not in one_data_df.columns:\n",
    "                one_data_df = one_data_df.withColumn('f', py_f.col(\"Feed\"))\n",
    "            if data_df is None:\n",
    "                data_df=one_data_df\n",
    "            else:\n",
    "                data_df=data_df.union(one_data_df)\n",
    "            data_df = data_df.withColumn('FeedType', py_f.lit(feed_type))\\\n",
    "            .select('FeedType','Feed','f','Product',col_map['bid'],col_map['ask'],col_map['timestamp'])\\\n",
    "            .groupBy('FeedType','Feed','f','Product',col_map['timestamp']).agg(\n",
    "                 py_f.round(py_f.max(col_map['bid']),3).alias(f'best_bid_{data_label}')\n",
    "                ,py_f.round(py_f.min(col_map['ask']),3).alias(f'best_ask_{data_label}')\n",
    "            ).withColumnRenamed('FeedType', f'FeedType_{data_label}')\\\n",
    "            .withColumnRenamed('Feed', f'Feed_{data_label}')\\\n",
    "            .withColumnRenamed(col_map['timestamp'], f'exchange_timestamp_{data_label}')\\\n",
    "            .withColumnRenamed('f', f'f_{data_label}')\\\n",
    "            .withColumn(f\"mid_{data_label}\",(py_f.col(f'best_ask_{data_label}')-py_f.col(f'best_bid_{data_label}'))/py_f.lit(2))\\\n",
    "            .withColumn(f\"bid_ask_{data_label}\",(py_f.col(f'best_ask_{data_label}')-py_f.col(f'best_bid_{data_label}'))/py_f.col(f\"mid_{data_label}\")) \\\n",
    "            .withColumn(f'timestamp_ts_utc_{data_label}',py_f.from_unixtime(py_f.col(f'exchange_timestamp_{data_label}')/1000/1000/1000))\\\n",
    "            .withColumn(f'timestamp_ts_est_{data_label}',py_f.from_utc_timestamp((py_f.from_unixtime(py_f.col(f'exchange_timestamp_{data_label}')/1000/1000/1000)),'America/New_York'))\n",
    "        part_by = [f'FeedType_{data_label}',\"Product\"]\n",
    "        self.raw_df[f\"{data_label}\"]=data_df.repartition(*part_by)\n",
    "    def dv_universe(self):\n",
    "        dv_key='universe_check'\n",
    "        self.data_validation[dv_key]={}\n",
    "        for one_key in self.raw_df.keys():\n",
    "            one_df = self.raw_df.get(one_key)\n",
    "            col_name = f\"{one_key}_ticker_count\"\n",
    "            curr_count = one_df.agg(py_f.countDistinct(\"Product\").alias(col_name)).collect()\n",
    "            curr_count =    [i.__getitem__(col_name) for i in curr_count][0]\n",
    "            self.data_validation[dv_key][one_key]=curr_count\n",
    "    \n",
    "    def dv_ts_unique(self):\n",
    "        dv_key='ts_unique_check'\n",
    "        self.data_validation[dv_key]={}\n",
    "        for one_key in self.raw_df.keys():\n",
    "            one_df = self.raw_df.get(one_key)\n",
    "            col_map=self.column_map.get(one_key)\n",
    "            ts_field=col_map.get('timestamp')\n",
    "            seq_field=col_map.get('seq_number')\n",
    "            count_alias,countDistinct_alias = f'count_{ts_field}',f'countDistinct_{ts_field}'\n",
    "            uniq_ts_check = one_df.groupBy(col_map.get('partition_by')).agg(\n",
    "                py_f.count(ts_field).alias(count_alias),py_f.countDistinct(ts_field,seq_field).alias(countDistinct_alias)\n",
    "            ).where(f'{count_alias}>{countDistinct_alias}').count()\n",
    "            self.data_validation[dv_key][one_key]=uniq_ts_check\n",
    "    def set_volume_ptile(self):\n",
    "        by_prod_feed=self.raw_df_prepped[\"mt_roundlot_nbbo\"].groupBy('Product').count().orderBy('Product')\n",
    "        by_prod_feed=by_prod_feed.select(\"Product\",'count', \n",
    "            py_f.round((py_f.floor(py_f.percent_rank().over( Window.partitionBy().orderBy(by_prod_feed['count']))/py_f.lit(self.round_factor))*py_f.lit(self.round_factor)),1).alias(\"update_count_pctrank\"))\n",
    "        by_prod_feed=by_prod_feed.withColumn('volume_level',py_f.when(py_f.col('update_count_pctrank')==0.0,'low')\\\n",
    "                                                                                .otherwise(py_f.when(py_f.col('update_count_pctrank')==0.3,'moderate').otherwise('high'))).cache()\n",
    "        #by_prod_feed.groupBy('update_count_pctrank').count()\n",
    "        self.volume_rank_df = by_prod_feed\n",
    "\n",
    "    def set_common_universe(self):\n",
    "        bbo_nbbo_cols = ['Product','Feed','dt','f','bidask_spread_timew_avg','data_count','is_trading_hours','hour_est']\n",
    "        df1=self.stats_df['mt_roundlot_bbo_stats_agg'].select(bbo_nbbo_cols)\\\n",
    "        .withColumnRenamed('bidask_spread_timew_avg',f'bidask_spread_timew_avg_bbo_roundlot').withColumnRenamed('data_count',f'data_count_bbo_roundlot')\n",
    "        df2=self.stats_df['mt_roundlot_nbbo_stats_agg'].select(bbo_nbbo_cols)\\\n",
    "        .withColumnRenamed('bidask_spread_timew_avg',f'bidask_spread_timew_avg_nbbo_roundlot').withColumnRenamed('data_count',f'data_count_nbbo_roundlot')\n",
    "        temp_df = df1.join(df2\n",
    "                 ,(df1.Product==df2.Product)\n",
    "                 & (df1.Feed==df2.Feed)\n",
    "                 & (df1.dt==df2.dt)\n",
    "                 & (df1.f==df2.f)\n",
    "                 & (df1.is_trading_hours==df2.is_trading_hours)\n",
    "                 & (df1.hour_est==df2.hour_est)\n",
    "                ).drop(df2.Product).drop(df2.Feed).drop(df2.dt).drop(df2.f).drop(df2.is_trading_hours).drop(df2.hour_est)\n",
    "\n",
    "        odd_lot_cols = ['Product','dt','bidask_spread_timew_avg','FeedType','Feed','f','data_count','is_trading_hours','hour_est']\n",
    "        df3 = self.stats_df['mt_oddlot_stats_agg'].select(odd_lot_cols)\\\n",
    "                .withColumnRenamed('bidask_spread_timew_avg',f'bidask_spread_timew_avg_oddlot')\\\n",
    "                .withColumnRenamed('data_count',f'data_count_oddlot')\\\n",
    "                .withColumnRenamed('Feed',f'Feed_oddlot')\\\n",
    "                .withColumnRenamed('f',f'f_oddlot')\n",
    "        final_df=temp_df.join(df3\n",
    "                 ,(temp_df.Product==df3.Product)\n",
    "                 & (temp_df.dt==df3.dt)\n",
    "                 & (temp_df.is_trading_hours==df3.is_trading_hours)\n",
    "                 & (temp_df.hour_est==df3.hour_est)\n",
    "                             ).drop(df3.Product).drop(df3.dt).drop(temp_df.is_trading_hours).drop(temp_df.hour_est)\n",
    "        volume_rank_df = self.volume_rank_df.select('Product','update_count_pctrank')\n",
    "        final_df=final_df.join(volume_rank_df\n",
    "                              ,(volume_rank_df.Product==final_df.Product)).drop(volume_rank_df.Product)\n",
    "        self.stats_df['all_by_symbol_feed_date']=final_df.cache()\n",
    "        \n",
    "    def join_dfs(self,dl1,dl2):\n",
    "        df1=self.raw_df_prepped[dl1]\n",
    "        df2=self.raw_df_prepped[dl2]\n",
    "        df1_df2=df1.join(df2, (df1.Product==df2.Product) &  (df1[f'exchange_timestamp_{dl1}']==df2[f'exchange_timestamp_{dl2}']),'inner').drop(df2.Product)\n",
    "        df1_df2=df1_df2.withColumn('exchange_timestamp',py_f.when(py_f.col(f'exchange_timestamp_{dl1}').isNull(), py_f.col(f'exchange_timestamp_{dl2}')).otherwise(py_f.col(f'exchange_timestamp_{dl1}')))\n",
    "        self.joined_df[f\"{dl1}_{dl2}\"]=df1_df2.cache()\n",
    "    [\n",
    "    'FeedType_mt_oddlot', 'Feed_mt_oddlot', 'f_mt_oddlot', 'Product'\n",
    "    , 'exchange_timestamp_mt_oddlot', 'best_bid_mt_oddlot', 'best_ask_mt_oddlot', 'bid_ask_mt_oddlot'\n",
    "    , 'timestamp_ts_utc_mt_oddlot', 'timestamp_ts_est_mt_oddlot'\n",
    "    , 'FeedType_mt_roundlot_bbo', 'Feed_mt_roundlot_bbo', 'f_mt_roundlot_bbo'\n",
    "    , 'exchange_timestamp_mt_roundlot_bbo', 'best_bid_mt_roundlot_bbo', 'best_ask_mt_roundlot_bbo', 'bid_ask_mt_roundlot_bbo'\n",
    "    , 'timestamp_ts_utc_mt_roundlot_bbo', 'timestamp_ts_est_mt_roundlot_bbo', 'exchange_timestamp'\n",
    "    ]\n",
    "    def calc_timew_spread_paired(self,dl1,dl2):\n",
    "        part_cols = [f'FeedType_{dl1}', f'Feed_{dl1}', f'f_{dl1}', 'Product', f'FeedType_{dl2}', f'Feed_{dl2}', f'f_{dl2}']\n",
    "        joined_df=self.joined_df[f\"{dl1}_{dl2}\"]\n",
    "        joined_df=joined_df.withColumn('time_est', py_f.date_format(f'timestamp_ts_est_{dl1}', 'HH:mm:ss'))\\\n",
    "                   .withColumn('hour_est', py_f.date_format(f'timestamp_ts_est_{dl1}', 'HH'))\\\n",
    "                    .withColumn('is_trading_hours', ((py_f.col('time_est')>=py_f.lit('09:30:00')) & (py_f.col('time_est')<=py_f.lit('15:59:00'))))\n",
    "        prev_window = Window.partitionBy(*[part_cols]).orderBy(py_f.col('exchange_timestamp'))\n",
    "        joined_df = joined_df.withColumn(\"prev_exchange_timestamp\", py_f.lag(py_f.col('exchange_timestamp')).over(prev_window))\n",
    "        joined_df = joined_df.withColumn(\"diff_exchange_timestamp\",joined_df.exchange_timestamp-joined_df.prev_exchange_timestamp)\n",
    "        joined_df = joined_df.withColumn(f\"bidask_timeweight_{dl1}\",joined_df[f'bid_ask_{dl1}']*joined_df.diff_exchange_timestamp)    \n",
    "        joined_df = joined_df.withColumn(f\"bidask_timeweight_{dl2}\",joined_df[f'bid_ask_{dl2}']*joined_df.diff_exchange_timestamp)     \n",
    "        #df_vol_rank=self.volume_rank_df.drop('update_count_pctrank').drop('count')\n",
    "        #joined_df = df_vol_rank.join(py_f.broadcast(joined_df),(df_vol_rank.Product==joined_df.Product)).drop(df_vol_rank.Product)\n",
    "        self.stats_df[f'joined_df_{dl1}_{dl2}']=joined_df\n",
    "        joined_df_stats_by_symbol=joined_df.groupBy('Product',f'Feed_{dl2}',f'f_{dl2}','is_trading_hours','hour_est').agg(\n",
    "             py_f.sum(py_f.col('diff_exchange_timestamp')).alias('diff_exchange_timestamp_sum')\n",
    "            ,py_f.sum(py_f.col(f'bidask_timeweight_{dl1}')).alias(f'bidask_timeweight_{dl1}_sum')\n",
    "            ,py_f.sum(py_f.col(f'bidask_timeweight_{dl2}')).alias(f'bidask_timeweight_{dl2}_sum')\n",
    "        ).withColumn(f'bid_ask_tw_{dl1}',py_f.col(f'bidask_timeweight_{dl1}_sum')/py_f.col('diff_exchange_timestamp_sum'))\\\n",
    "        .withColumn(f'bid_ask_tw_{dl2}',py_f.col(f'bidask_timeweight_{dl2}_sum')/py_f.col('diff_exchange_timestamp_sum'))\\\n",
    "        .orderBy('Product',f'Feed_{dl2}',f'f_{dl2}','is_trading_hours').cache()\n",
    "        self.stats_df[f'joined_df_stats_by_symbol_{dl1}_{dl2}']=joined_df_stats_by_symbol\n",
    "        \n",
    "        joined_df_stats_by_trading_hour=joined_df_stats_by_symbol.groupBy(f'Feed_{dl2}',f'f_{dl2}','is_trading_hours','hour_est')\\\n",
    "        .agg(\n",
    "             py_f.mean(py_f.col(f'bid_ask_tw_{dl1}'))\n",
    "            ,py_f.mean(py_f.col(f'bid_ask_tw_{dl2}'))\n",
    "            ,py_f.count(py_f.col(f'bid_ask_tw_{dl1}'))\n",
    "            ,py_f.count(py_f.col(f'bid_ask_tw_{dl2}'))\n",
    "        )\n",
    "        self.stats_df[f'joined_df_stats_by_trading_hour_{dl1}_{dl2}']=joined_df_stats_by_trading_hour\n",
    "        \n",
    "    def calc_timew_spread(self,data_label):\n",
    "        col_map=self.column_map.get(data_label)\n",
    "        l_df =  self.raw_df.get(data_label)\n",
    "        l_df = l_df.withColumn('timestamp_ts_utc',py_f.from_unixtime(py_f.col(col_map.get('timestamp'))/1000/1000/1000))\\\n",
    "                   .withColumn('timestamp_ts_est',py_f.from_utc_timestamp((py_f.from_unixtime(py_f.col(col_map.get('timestamp'))/1000/1000/1000)),'America/New_York'))\\\n",
    "                   .withColumn('time_est', py_f.date_format('timestamp_ts_est', 'HH:mm:ss'))\\\n",
    "                   .withColumn('hour_est', py_f.date_format('timestamp_ts_est', 'HH'))\\\n",
    "                   .withColumn('is_trading_hours', ((py_f.col('time_est')>=py_f.lit('09:30:00'))&(py_f.col('time_est')<=py_f.lit('15:59:00'))))\n",
    "        l_df = l_df.withColumn(\"bid_ask\",(py_f.col(col_map.get('ask'))-py_f.col(col_map.get('bid')))/py_f.col(col_map.get('bid')) )\n",
    "        prev_window = Window.partitionBy(*col_map.get('partition_by')).\\\n",
    "                        orderBy(py_f.col(col_map.get('timestamp')),py_f.col(col_map.get('seq_number')),l_df.bid_ask.desc())\n",
    "        l_df = l_df.withColumn(\"next_LastReceiptTimestamp\", py_f.lead(py_f.col(col_map.get('timestamp'))).over(prev_window))\n",
    "        l_df = l_df.withColumn(\"diff_LastReceiptTimestamp\",py_f.col(col_map.get('timestamp'))-l_df.next_LastReceiptTimestamp)\n",
    "        l_df = l_df.withColumn(\"bidask_timeweight\",l_df.bid_ask*l_df.diff_LastReceiptTimestamp)\n",
    "        bid_ask_agg= l_df.where('diff_LastReceiptTimestamp is not null and bid_ask<100').groupby(*col_map.get('partition_by')).\\\n",
    "                agg(py_f.sum('diff_LastReceiptTimestamp').alias('time_sum'),\n",
    "                    py_f.sum('bidask_timeweight').alias('bidask_timeweight_sum'),\n",
    "                    py_f.count(py_f.lit(1)).alias('data_count'))\n",
    "        bid_ask_agg=bid_ask_agg.withColumn(\"bidask_spread_timew_avg\",bid_ask_agg.bidask_timeweight_sum/bid_ask_agg.time_sum)  \n",
    "        self.stats_df[f\"{data_label}_stats_intermediate\"]=l_df\n",
    "        self.stats_df[f\"{data_label}_stats_agg\"]=bid_ask_agg\n",
    "        self.stats_df[f\"{data_label}_stats_agg_final\"]=bid_ask_agg.agg(py_f.mean(bid_ask_agg.bidask_spread_timew_avg).alias('bidask_mean_timew'),\n",
    "                                                                       py_f.expr('percentile(bidask_spread_timew_avg, array(0.5))').alias('bidask_median_timew'),\n",
    "                                                                        py_f.sum(bid_ask_agg.data_count).alias('data_count'))\n",
    "            \n",
    "mt_roundlot=MtRoundLot(7) \n",
    "print('mt_oddlot')\n",
    "mt_roundlot.set_data(\"mt_oddlot\")\n",
    "print('mt_roundlot_bbo')\n",
    "mt_roundlot.set_data(\"mt_roundlot_bbo\")\n",
    "print('mt_roundlot_nbbo')\n",
    "mt_roundlot.set_data(\"mt_roundlot_nbbo\")\n",
    "#\n",
    "if True:\n",
    "    print('mt_oddlot_prepped')\n",
    "    mt_roundlot.set_data_prepped(\"mt_oddlot\")\n",
    "    print('mt_roundlot_bbo_prepped')\n",
    "    mt_roundlot.set_data_prepped(\"mt_roundlot_bbo\")\n",
    "    print('mt_roundlot_nbbo_prepped')\n",
    "    mt_roundlot.set_data_prepped(\"mt_roundlot_nbbo\")\n",
    "\n",
    "mt_roundlot.set_volume_ptile()\n",
    "mt_roundlot.join_dfs('mt_oddlot','mt_roundlot_bbo')\n",
    "mt_roundlot.join_dfs('mt_oddlot','mt_roundlot_nbbo')\n",
    "\n",
    "#if False:\n",
    "#    mt_roundlot.dv_universe()\n",
    "#    mt_roundlot.dv_ts_unique()\n",
    "\n",
    "mt_roundlot.calc_timew_spread_paired(\"mt_oddlot\",\"mt_roundlot_bbo\")\n",
    "mt_roundlot.calc_timew_spread_paired(\"mt_oddlot\",\"mt_roundlot_nbbo\")\n",
    "#mt_roundlot.calc_timew_spread(\"mt_roundlot_bbo\")\n",
    "#mt_roundlot.calc_timew_spread(\"mt_roundlot_nbbo\")\n",
    "#mt_roundlot.set_common_universe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-20T16:24:38.867464Z",
     "iopub.status.idle": "2023-03-20T16:24:38.867691Z",
     "shell.execute_reply": "2023-03-20T16:24:38.867585Z",
     "shell.execute_reply.started": "2023-03-20T16:24:38.867574Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for one_result in [ 'joined_df_stats_by_symbol_mt_oddlot_mt_roundlot_nbbo'\n",
    "                       ,'joined_df_stats_by_symbol_mt_oddlot_mt_roundlot_bbo'\n",
    "                       ,'joined_df_stats_by_trading_hour_mt_oddlot_mt_roundlot_bbo'\n",
    "                       ,'joined_df_stats_by_trading_hour_mt_oddlot_mt_roundlot_bbo']:\n",
    "        mt_roundlot.stats_df[one_result].write.option(\"header\",True).mode(\"overwrite\").parquet(f\"{mt_roundlot.s3_dir_root_results}/{one_result}.parquet\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:34:03.311578Z",
     "iopub.status.busy": "2023-03-20T15:34:03.311416Z",
     "iopub.status.idle": "2023-03-20T16:24:38.856907Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o594.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.FetchFailedException\n\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1166)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:904)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 3), which maintains the block data to fetch is dead.\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1136)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1128)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1001)\n\t... 25 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1995)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:467)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt_roundlot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mt_roundlot_bbo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o594.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.FetchFailedException\n\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1166)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:904)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 3), which maintains the block data to fetch is dead.\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1136)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1128)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1001)\n\t... 25 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1995)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:467)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "mt_roundlot.raw_df['mt_roundlot_bbo'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:25:52.711961Z",
     "iopub.status.busy": "2022-10-19T00:25:52.711099Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3110b7bad8ce4ef4927f6ce825ce72c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326ae2f55c3744e388a0b031884ed609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt_roundlot.joined_df['mt_oddlot_mt_roundlot_bbo'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "-----------------------------------------\n",
    "-----------------------------------------\n",
    "the cell below:\n",
    "1. the first If statement writes repartitioned parquet file if activated. experiment 7 is what currently used as input data\n",
    "2. the second If, when activated, produces .csv files that feed tables and charts in the blog:\n",
    "a)ol_rl_nbbo<_pivot>.csv - odd lots vs round lots on NBBO\n",
    "b)ol_rl_bbo<_pivot>.csv - odd lots vs round lots on BBO\n",
    "3. the rest of the data is picked up and produced by mt_roundlot_study_post_analysis.ipynb. both notebook need to be combined into singhle clean and visual notebook that runs end-to-end\n",
    "4. graph data is available: s3://maystreetdata/analysis/blog_graphs/\n",
    "-----------------------------------------\n",
    "-----------------------------------------\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:53.365342Z",
     "iopub.status.busy": "2022-10-19T00:07:53.364708Z",
     "iopub.status.idle": "2022-10-19T00:07:53.651406Z",
     "shell.execute_reply": "2022-10-19T00:07:53.650362Z",
     "shell.execute_reply.started": "2022-10-19T00:07:53.365303Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38ec6133e4d4de9a4a4f4cef82dbef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d5a4fd98cf47a4b19582a01fbc7b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    for one_feed_type in ['mt_oddlot','mt_roundlot_bbo','mt_roundlot_nbbo']:\n",
    "        print(one_feed_type)\n",
    "        #part_by = [f\"FeedType_{one_feed_type}\",\"Product\",f\"Feed_{one_feed_type}\",f'f_{one_feed_type}']\n",
    "        part_by = [f'FeedType_{one_feed_type}',\"Product\"]\n",
    "        mt_roundlot.raw_df[one_feed_type]\\\n",
    "                    .write\\\n",
    "                    .option(\"header\",True) \\\n",
    "                    .partitionBy(*part_by) \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .parquet(f\"{mt_roundlot.s3_dir_partition_experiments}/{one_feed_type}.parquet\")\n",
    "\n",
    "if False:\n",
    "    sort_val=['Product','Feed_mt_roundlot_nbbo','f_mt_roundlot_nbbo','hour_est']\n",
    "    ol_nbbo_pd=mt_roundlot.stats_df[f'joined_df_stats_by_symbol_mt_oddlot_mt_roundlot_nbbo'].toPandas()\\\n",
    "    .drop(columns=['diff_exchange_timestamp_sum','bidask_timeweight_mt_oddlot_sum','bidask_timeweight_mt_roundlot_nbbo_sum'])\n",
    "    ol_nbbo_pd['ol_rl_comp']= (ol_nbbo_pd['bid_ask_tw_mt_oddlot']-ol_nbbo_pd['bid_ask_tw_mt_roundlot_nbbo']).astype(float)\n",
    "    ol_nbbo_pd['is_trading_hours']=ol_nbbo_pd['is_trading_hours']*1\n",
    "    ol_nbbo_pd.sort_values(['Product','Feed_mt_roundlot_nbbo','f_mt_roundlot_nbbo','hour_est']).to_csv(f\"s3://maystreetdata/analysis/{f'ol_rl_nbbo.csv'}\")\n",
    "    pd.pivot_table(ol_nbbo_pd,values=['ol_rl_comp'],index=['volume_level','Product','Feed_mt_roundlot_nbbo','f_mt_roundlot_nbbo'],columns=['hour_est','is_trading_hours'])\\\n",
    "    .to_csv(f\"s3://maystreetdata/analysis/{f'ol_rl_nbbo_pivot.csv'}\")\n",
    "\n",
    "    ol_bbo_pd=mt_roundlot.stats_df[f'joined_df_stats_by_symbol_mt_oddlot_mt_roundlot_bbo'].toPandas()\\\n",
    "    .drop(columns=['diff_exchange_timestamp_sum','bidask_timeweight_mt_oddlot_sum','bidask_timeweight_mt_roundlot_bbo_sum'])\n",
    "    ol_bbo_pd['ol_rl_comp']= (ol_bbo_pd['bid_ask_tw_mt_oddlot']-ol_bbo_pd['bid_ask_tw_mt_roundlot_bbo']).astype(float)\n",
    "    ol_bbo_pd['is_trading_hours']=ol_bbo_pd['is_trading_hours']*1\n",
    "    ol_bbo_pd.sort_values(['Product','Feed_mt_roundlot_bbo','f_mt_roundlot_bbo','hour_est']).to_csv(f\"s3://maystreetdata/analysis/{f'ol_rl_bbo.csv'}\")\n",
    "    pd.pivot_table(ol_bbo_pd,values=['ol_rl_comp'],index=['volume_level','Product','Feed_mt_roundlot_bbo','f_mt_roundlot_bbo'],columns=['hour_est','is_trading_hours'])\\\n",
    "    .to_csv(f\"s3://maystreetdata/analysis/{f'ol_rl_bbo_pivot.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:53.654335Z",
     "iopub.status.busy": "2022-10-19T00:07:53.653804Z",
     "iopub.status.idle": "2022-10-19T00:07:53.880420Z",
     "shell.execute_reply": "2022-10-19T00:07:53.879238Z",
     "shell.execute_reply.started": "2022-10-19T00:07:53.654276Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7c1f36cbcc46c1a9892208ce2a1b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25ab65e894b4c21b1fb648d1a1fa0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    for one_lbl in ['oddlot','roundlot_bbo','roundlot_nbbo']:\n",
    "        odd_lot_sample = mt_roundlot.stats_df[f\"mt_{one_lbl}_stats_intermediate\"].\\\n",
    "        where(f\"is_trading_hours==True  and bid_ask>0.00\")\n",
    "        odd_lot_sample_t=odd_lot_sample.join(mt_roundlot.volume_rank_df,\n",
    "                           (odd_lot_sample.Product==mt_roundlot.volume_rank_df.Product)).drop(mt_roundlot.volume_rank_df.Product).drop(py_f.col('count'))\n",
    "        odd_lot_sample_t.where(f\"update_count_pctrank >0.5 \").groupBy('hour_est')\\\n",
    "        .agg(py_f.mean(odd_lot_sample_t.bid_ask).alias('bidask_mean'),py_f.count(odd_lot_sample_t.update_count_pctrank).alias('data_count'))\\\n",
    "        .orderBy('hour_est').show(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:53.883524Z",
     "iopub.status.busy": "2022-10-19T00:07:53.883169Z",
     "iopub.status.idle": "2022-10-19T00:07:54.157665Z",
     "shell.execute_reply": "2022-10-19T00:07:54.156705Z",
     "shell.execute_reply.started": "2022-10-19T00:07:53.883485Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca6bcb1382544edbee4c6ac0567eeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    oddlot_df=mt_roundlot.raw_df[\"mt_oddlot\"].\\\n",
    "    where(f\"Product=='A'\")\\\n",
    "    .select('Feed','f','Product','BidPrice_1','AskPrice_1','LastExchangeTimestamp','LastSequenceNumber')\\\n",
    "    .groupBy('Feed','f','Product','LastExchangeTimestamp').agg(py_f.round(py_f.max('BidPrice_1'),2).alias('best_bid_ol'),py_f.round(py_f.min('AskPrice_1'),2).alias('best_ask_ol'))\\\n",
    "    .withColumn('timestamp_ts_utc',py_f.from_unixtime(py_f.col('LastExchangeTimestamp')/1000/1000/1000))\\\n",
    "    .withColumn('timestamp_ts_est',py_f.from_utc_timestamp((py_f.from_unixtime(py_f.col('LastExchangeTimestamp')/1000/1000/1000)),'America/New_York'))\\\n",
    "    .where(\"timestamp_ts_est >= '2022-08-22 09:30:00'\").orderBy('Product','LastExchangeTimestamp')\n",
    "    oddlot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:54.165898Z",
     "iopub.status.busy": "2022-10-19T00:07:54.165461Z",
     "iopub.status.idle": "2022-10-19T00:07:54.574556Z",
     "shell.execute_reply": "2022-10-19T00:07:54.573512Z",
     "shell.execute_reply.started": "2022-10-19T00:07:54.165856Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126d6c68220d4bdcb4e91a14434a4ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FeedType_mt_roundlot_bbo', 'Feed_mt_roundlot_bbo', 'f_mt_roundlot_bbo', 'Product', 'exchange_timestamp_mt_roundlot_bbo', 'best_bid_mt_roundlot_bbo', 'best_ask_mt_roundlot_bbo', 'bid_ask_mt_roundlot_bbo', 'timestamp_ts_utc_mt_roundlot_bbo', 'timestamp_ts_est_mt_roundlot_bbo']"
     ]
    }
   ],
   "source": [
    "mt_roundlot.raw_df[\"mt_roundlot_bbo\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:54.576293Z",
     "iopub.status.busy": "2022-10-19T00:07:54.576003Z",
     "iopub.status.idle": "2022-10-19T00:07:54.973145Z",
     "shell.execute_reply": "2022-10-19T00:07:54.972266Z",
     "shell.execute_reply.started": "2022-10-19T00:07:54.576263Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056a10e5613e47118f2f23c66a1d01fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    roundlot_bbo_df=mt_roundlot.raw_df[\"mt_roundlot_bbo\"].\\\n",
    "    where(f\"Product=='A'\")\\\n",
    "    .select('Feed','Product','BidPrice','AskPrice','ExchangeTimestamp','SequenceNumber')\\\n",
    "    .groupBy('Feed','Product','ExchangeTimestamp').agg(py_f.round(py_f.max('BidPrice'),2).alias('best_bid_rl'),py_f.round(py_f.min('AskPrice'),2).alias('best_ask_rl'))\\\n",
    "    .withColumn('timestamp_ts_utc',py_f.from_unixtime(py_f.col('ExchangeTimestamp')/1000/1000/1000))\\\n",
    "    .withColumn('timestamp_ts_est',py_f.from_utc_timestamp((py_f.from_unixtime(py_f.col('ExchangeTimestamp')/1000/1000/1000)),'America/New_York'))\\\n",
    "    .where(\"timestamp_ts_est >= '2022-08-22 09:30:00'\")\n",
    "\n",
    "    roundlot_bbo_df.orderBy('Product','ExchangeTimestamp').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:54.975064Z",
     "iopub.status.busy": "2022-10-19T00:07:54.974463Z",
     "iopub.status.idle": "2022-10-19T00:07:55.563902Z",
     "shell.execute_reply": "2022-10-19T00:07:55.563068Z",
     "shell.execute_reply.started": "2022-10-19T00:07:54.975026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ddd94b385141d2b8134953314cc2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    df_ol=mt_roundlot.raw_df[\"mt_oddlot\"].where(\"Product=='A'\")\n",
    "    df_ol.show()\n",
    "    df_rl_bbo=mt_roundlot.raw_df[\"mt_roundlot_bbo\"].where(\"Product=='A'\")\n",
    "    df_rl_bbo.show()\n",
    "\n",
    "    round_odd_lot_df=df_ol.join(df_rl_bbo,\n",
    "                        (df_ol.Product==df_rl_bbo.Product) \n",
    "                         & (df_ol.exchange_timestamp_mt_oddlot==df_rl_bbo.exchange_timestamp_mt_roundlot_bbo) ,'outer')\\\n",
    "    .withColumn('exchange_timestamp',py_f.when(py_f.col('exchange_timestamp_mt_oddlot').isNull(), py_f.col('exchange_timestamp_mt_roundlot_bbo')).otherwise(py_f.col('exchange_timestamp_mt_oddlot')))\n",
    "\n",
    "    round_odd_lot_df.where(\"exchange_timestamp_mt_oddlot is not null and exchange_timestamp_mt_roundlot_bbo is not null \").orderBy('exchange_timestamp').agg(\n",
    "        py_f.count('exchange_timestamp_mt_oddlot')\n",
    "        ,py_f.count('exchange_timestamp_mt_roundlot_bbo')\n",
    "        ,py_f.count('exchange_timestamp')\n",
    "    )\n",
    "    round_odd_lot_df\\\n",
    "    .where(\"exchange_timestamp_mt_oddlot is not null and exchange_timestamp_mt_roundlot_bbo is not null\")\\\n",
    "    .agg(py_f.count('exchange_timestamp_mt_oddlot'),py_f.count('exchange_timestamp_mt_roundlot_bbo'),py_f.count('exchange_timestamp')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:55.567914Z",
     "iopub.status.busy": "2022-10-19T00:07:55.567536Z",
     "iopub.status.idle": "2022-10-19T00:07:55.793065Z",
     "shell.execute_reply": "2022-10-19T00:07:55.792223Z",
     "shell.execute_reply.started": "2022-10-19T00:07:55.567873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e0255a7d9841c4a5937a0e7c7a9c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    lm=0.00\n",
    "    l_df=mt_roundlot.stats_df['all_by_symbol_feed_date']\n",
    "    l_df.where(f\"Product=='ABC' and  bidask_spread_timew_avg_bbo_roundlot > {lm} \").orderBy('Product', 'dt', 'Feed', 'f','data_count_oddlot').show(500)\n",
    "    l_df.select('data_count_oddlot').rdd.flatMap(lambda x: x).histogram(100) ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:55.858633Z",
     "iopub.status.busy": "2022-10-19T00:07:55.849198Z",
     "iopub.status.idle": "2022-10-19T00:07:56.090862Z",
     "shell.execute_reply": "2022-10-19T00:07:56.089988Z",
     "shell.execute_reply.started": "2022-10-19T00:07:55.858590Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0075c3f34c344710a379f351cbd08bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    agg_1=mt_roundlot.stats_df['all_by_symbol_feed_date'].groupBy('Product','is_trading_hours','Feed_oddlot','f_oddlot','update_count_pctrank','hour_est').agg(\n",
    "     py_f.expr('percentile(bidask_spread_timew_avg_bbo_roundlot, 0.5)').alias('bid_ask_round_lot_bbo_median')\n",
    "    ,py_f.expr('percentile(bidask_spread_timew_avg_nbbo_roundlot, 0.5)').alias('bid_ask_round_lot_nbbo_median')\n",
    "    ,py_f.expr('percentile(bidask_spread_timew_avg_oddlot, 0.5)').alias('bid_ask_oddlot_median')\n",
    "\n",
    "\n",
    "    ,py_f.mean('bidask_spread_timew_avg_bbo_roundlot').alias('bid_ask_round_lot_bbo_mean')\n",
    "    ,py_f.mean('bidask_spread_timew_avg_nbbo_roundlot').alias('bid_ask_round_lot_nbbo_mean')\n",
    "    ,py_f.mean('bidask_spread_timew_avg_oddlot').alias('bid_ask_oddlot_mean')\n",
    "    ,py_f.mean('data_count_oddlot').alias('oddlot_data_count_mean')\n",
    "    ).orderBy('Product','is_trading_hours')\n",
    "    agg_1=agg_1\\\n",
    "    .withColumn(\"oddlot_bbo_mean\",py_f.col('bid_ask_oddlot_mean')>=py_f.col('bid_ask_round_lot_bbo_mean'))\\\n",
    "    .withColumn(\"oddlot_nbbo_mean\",py_f.col('bid_ask_oddlot_mean')>=py_f.col('bid_ask_round_lot_nbbo_mean'))\\\n",
    "    .withColumn(\"oddlot_bbo_median\",py_f.col('bid_ask_oddlot_median')>=py_f.col('bid_ask_round_lot_bbo_median'))\\\n",
    "    .withColumn(\"oddlot_nbbo_median\",py_f.col('bid_ask_oddlot_median')>=py_f.col('bid_ask_round_lot_nbbo_median'))\n",
    "    #agg_1.agg(py_f.mean('bid_ask_round_lot_bbo'),py_f.mean('bid_ask_round_lot_nbbo'),py_f.mean('bid_ask_oddlot')).show()\n",
    "    agg_pd=agg_1.orderBy('Product','is_trading_hours','hour_est').toPandas()\n",
    "    agg_pd\n",
    "    agg_pd.to_csv(f\"s3://maystreetdata/analysis/{f'agg_pd.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:56.092463Z",
     "iopub.status.busy": "2022-10-19T00:07:56.092045Z",
     "iopub.status.idle": "2022-10-19T00:07:56.375987Z",
     "shell.execute_reply": "2022-10-19T00:07:56.375127Z",
     "shell.execute_reply.started": "2022-10-19T00:07:56.092434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf116f9ed74e20bc7434a6b9ff27d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from scipy import stats \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:56.379898Z",
     "iopub.status.busy": "2022-10-19T00:07:56.379625Z",
     "iopub.status.idle": "2022-10-19T00:07:56.651212Z",
     "shell.execute_reply": "2022-10-19T00:07:56.650175Z",
     "shell.execute_reply.started": "2022-10-19T00:07:56.379870Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29358e7aa48041a8be53cd7b451f5afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    for one_key in ['mt_oddlot_stats_intermediate', 'mt_oddlot_stats_agg', 'mt_oddlot_stats_agg_final'\n",
    "                    , 'mt_roundlot_bbo_stats_intermediate', 'mt_roundlot_bbo_stats_agg'\n",
    "                    , 'mt_roundlot_bbo_stats_agg_final', 'mt_roundlot_nbbo_stats_intermediate'\n",
    "                    , 'mt_roundlot_nbbo_stats_agg', 'mt_roundlot_nbbo_stats_agg_final']:\n",
    "    #for one_key in ['mt_oddlot_stats_intermediate']:\n",
    "        if '_stats_intermediate' in one_key:\n",
    "            mt_roundlot.stats_df.get(one_key).where(f\"Product like 'AAP%' \")\\\n",
    "                    .repartition(1)\\\n",
    "                    .write\\\n",
    "                    .option(\"header\",True) \\\n",
    "                    .partitionBy(\"FeedType\",\"Product\",\"Feed\",'f',\"dt\")\\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .parquet(f\"{mt_roundlot.s3_dir_root_results}/{one_key}.parquet\")\n",
    "        else:\n",
    "            mt_roundlot.stats_df.get(one_key).repartition(1).write\\\n",
    "                    .option(\"header\",True) \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .parquet(f\"{mt_roundlot.s3_dir_root_results}/{one_key}.parquet\")\n",
    "\n",
    "#mt_oddlot_stats_pd=mt_roundlot.stats_df.get('mt_oddlot_stats_agg').toPandas()\n",
    "#mt_roundlot.stats_df.get('mt_oddlot_stats').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:56.652731Z",
     "iopub.status.busy": "2022-10-19T00:07:56.652473Z",
     "iopub.status.idle": "2022-10-19T00:07:56.897543Z",
     "shell.execute_reply": "2022-10-19T00:07:56.895842Z",
     "shell.execute_reply.started": "2022-10-19T00:07:56.652706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008410c8a9384b938f6299d11989b72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    key='oddlot'\n",
    "    mt_roundlot.stats_df.get(f'mt_{key}_stats_intermediate').where(f\"Product=='ABC' and Feed=='XDPV2' and dt=='2022-08-22' and f=='xdp_nyse_integrated' \")\\\n",
    "    .toPandas().sort_values(['Product','FeedType',\"Feed\",\"dt\",'f','MarketParticipant','LastExchangeTimestamp','LastSequenceNumber']).to_csv(f\"s3://maystreetdata/analysis/{f'{key}_sample_pd.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:56.949415Z",
     "iopub.status.busy": "2022-10-19T00:07:56.949065Z",
     "iopub.status.idle": "2022-10-19T00:07:57.168988Z",
     "shell.execute_reply": "2022-10-19T00:07:57.167979Z",
     "shell.execute_reply.started": "2022-10-19T00:07:56.949386Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404e6be47015402baaf5cbfa563065ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    mt_roundlot.stats_df.get(f'mt_{key}_stats_intermediate').where(f\"is_trading_hours=='false'\")\\\n",
    "           .select('timestamp_ts_utc','timestamp_ts_est','time_est','is_trading_hours').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:57.170988Z",
     "iopub.status.busy": "2022-10-19T00:07:57.170637Z",
     "iopub.status.idle": "2022-10-19T00:07:57.458282Z",
     "shell.execute_reply": "2022-10-19T00:07:57.457471Z",
     "shell.execute_reply.started": "2022-10-19T00:07:57.170949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781ee4882f054bba826c3e225364250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    mt_roundlot.stats_df.get(f'mt_{key}_stats_intermediate').where(f\"Product=='ABC' and Feed=='XDPV2' and dt=='2022-08-22' and f=='xdp_nyse_integrated' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:57.462008Z",
     "iopub.status.busy": "2022-10-19T00:07:57.461753Z",
     "iopub.status.idle": "2022-10-19T00:07:57.687477Z",
     "shell.execute_reply": "2022-10-19T00:07:57.686626Z",
     "shell.execute_reply.started": "2022-10-19T00:07:57.461980Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc955d99720d49d7aa2aae8722454804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "expected an indented block (<stdin>, line 3)\n",
      "  File \"<stdin>\", line 3\n",
      "    one_df = mt_roundlot.raw_df.get(one_key)\n",
      "         ^\n",
      "IndentationError: expected an indented block\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    for one_key in mt_roundlot.raw_df.keys():\n",
    "    one_df = mt_roundlot.raw_df.get(one_key)\n",
    "    break\n",
    "    #'FirstReceiptTimestamp', 'LastReceiptTimestamp', 'FirstExchangeTimestamp', 'LastExchangeTimestamp', 'FirstExchangeSendTimestamp', 'LastExchangeSendTimestamp',\n",
    "    dup_check = one_df.groupBy('Product','FeedType',\"Feed\",\"dt\",'f','MarketParticipant').agg(\n",
    "        py_f.count('FirstReceiptTimestamp').alias('FirstReceiptTimestamp'),py_f.countDistinct('FirstReceiptTimestamp','LastSequenceNumber').alias(f\"{'FirstReceiptTimestamp'}_dist\")\n",
    "        ,py_f.count('LastReceiptTimestamp').alias('LastReceiptTimestamp'),py_f.countDistinct('LastReceiptTimestamp','LastSequenceNumber').alias(f\"{'LastReceiptTimestamp'}_dist\")\n",
    "        ,py_f.count('FirstExchangeTimestamp').alias('FirstExchangeTimestamp'),py_f.countDistinct('FirstExchangeTimestamp','LastSequenceNumber').alias(f\"{'FirstExchangeTimestamp'}_dist\")\n",
    "        ,py_f.count('LastExchangeTimestamp').alias('LastExchangeTimestamp'),py_f.countDistinct('LastExchangeTimestamp','LastSequenceNumber').alias(f\"{'LastExchangeTimestamp'}_dist\")\n",
    "        ,py_f.count('FirstExchangeSendTimestamp').alias('FirstExchangeSendTimestamp'),py_f.countDistinct('FirstExchangeSendTimestamp','LastSequenceNumber').alias(f\"{'FirstExchangeSendTimestamp'}_dist\")\n",
    "        ,py_f.count('LastExchangeSendTimestamp').alias('LastExchangeSendTimestamp'),py_f.countDistinct('LastExchangeSendTimestamp','LastSequenceNumber').alias(f\"{'LastExchangeSendTimestamp'}_dist\")\n",
    "    )\n",
    "    dup_check\n",
    "    dup_check.where(f\"LastExchangeTimestamp>LastExchangeTimestamp_dist or LastExchangeSendTimestamp>LastExchangeSendTimestamp_dist\").sort('Product','FeedType',\"Feed\",\"dt\",'f','MarketParticipant').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:57.690021Z",
     "iopub.status.busy": "2022-10-19T00:07:57.689450Z",
     "iopub.status.idle": "2022-10-19T00:07:57.973969Z",
     "shell.execute_reply": "2022-10-19T00:07:57.973132Z",
     "shell.execute_reply.started": "2022-10-19T00:07:57.689991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301c2fe501564eb697fc8e44046ede1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_slice_to_parquet(df_l,l_fname):\n",
    "    df_l.repartition(1).write\\\n",
    "            .option(\"header\",True) \\\n",
    "            .partitionBy(\"FeedType\",\"Product\",\"Feed\",'f',\"dt\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(f\"{l_fname}\")\n",
    "if False:\n",
    "\n",
    "    if False:    \n",
    "        for one_feed in feeds:\n",
    "            one_feed_dets = list(one_feed)\n",
    "            feed_type,feed,f = one_feed_dets[0],one_feed_dets[1],one_feed_dets[2]\n",
    "            full_file_name = f\"{mt_roundlot.s3_dir_partition_experiments}/{feed_type}.parquet\"\n",
    "            write_df = mt_roundlot.raw_df.get('mt_oddlot').where(f\"FeedType=='{feed_type}' and Feed=='{feed}' and f=='{f}'\")\n",
    "            #print(f\"f:{feed},count:{write_df.count()}\")\n",
    "            print(feed_type,feed,f,\"\\n\\n\")    \n",
    "            save_slice_to_parquet(write_df,full_file_name)\n",
    "    else:\n",
    "        feed_type,feed,f ='f=bats_edgx', 'BatsPitch', 'BatsPitch'\n",
    "        full_file_name = f\"{mt_roundlot.s3_dir_partition_experiments}/{feed_type}.parquet\"\n",
    "        write_df = mt_roundlot.raw_df.get('mt_oddlot').where(f\"FeedType=='{feed_type}' and Feed=='{feed}' and f=='{f}' and Product like 'AA%' \")\n",
    "        #save_slice_to_parquet(write_df,full_file_name)\n",
    "        read_df = spark.read.parquet(full_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:57.976963Z",
     "iopub.status.busy": "2022-10-19T00:07:57.976701Z",
     "iopub.status.idle": "2022-10-19T00:07:58.381158Z",
     "shell.execute_reply": "2022-10-19T00:07:58.380324Z",
     "shell.execute_reply.started": "2022-10-19T00:07:57.976937Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a878ed67fe42689f1cd1432251f07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['FeedType_mt_roundlot_bbo', 'Feed_mt_roundlot_bbo', 'f_mt_roundlot_bbo', 'Product', 'exchange_timestamp_mt_roundlot_bbo', 'best_bid_mt_roundlot_bbo', 'best_ask_mt_roundlot_bbo', 'bid_ask_mt_roundlot_bbo', 'timestamp_ts_utc_mt_roundlot_bbo', 'timestamp_ts_est_mt_roundlot_bbo']\n",
      "\n",
      " ['FeedType_mt_roundlot_nbbo', 'Feed_mt_roundlot_nbbo', 'f_mt_roundlot_nbbo', 'Product', 'exchange_timestamp_mt_roundlot_nbbo', 'best_bid_mt_roundlot_nbbo', 'best_ask_mt_roundlot_nbbo', 'bid_ask_mt_roundlot_nbbo', 'timestamp_ts_utc_mt_roundlot_nbbo', 'timestamp_ts_est_mt_roundlot_nbbo']\n",
      "\n",
      " ['FeedType_mt_oddlot', 'Feed_mt_oddlot', 'f_mt_oddlot', 'Product', 'exchange_timestamp_mt_oddlot', 'best_bid_mt_oddlot', 'best_ask_mt_oddlot', 'bid_ask_mt_oddlot', 'timestamp_ts_utc_mt_oddlot', 'timestamp_ts_est_mt_oddlot']"
     ]
    }
   ],
   "source": [
    "print('\\n',mt_roundlot.raw_df.get('mt_roundlot_bbo').columns)\n",
    "print('\\n',mt_roundlot.raw_df.get('mt_roundlot_nbbo').columns)\n",
    "print('\\n',mt_roundlot.raw_df.get('mt_oddlot').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:07:58.449958Z",
     "iopub.status.busy": "2022-10-19T00:07:58.449516Z",
     "iopub.status.idle": "2022-10-19T00:07:58.950539Z",
     "shell.execute_reply": "2022-10-19T00:07:58.949407Z",
     "shell.execute_reply.started": "2022-10-19T00:07:58.449913Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cef1d7aa0a40a9a16008591551bb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'show'\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'NoneType' object has no attribute 'show'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    mt_roundlot.stats_df.get('mt_oddlot_stats_agg_final').show()\n",
    "    ,mt_roundlot.stats_df.get('mt_roundlot_nbbo_stats_agg_final').show()\n",
    "    ,mt_roundlot.stats_df.get('mt_roundlot_bbo_stats_agg_final').show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
