{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:25:02.409876Z",
     "iopub.status.busy": "2023-03-20T15:25:02.409460Z",
     "iopub.status.idle": "2023-03-20T15:26:09.637498Z",
     "shell.execute_reply": "2023-03-20T15:26:09.637430Z",
     "shell.execute_reply.started": "2023-03-20 15:25:02.414638+00:00"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-03-20 15:25:02,414.414 configure_magic] Magic cell payload received: {\"conf\": {\"spark.pyspark.python\": \"python3\", \"spark.pyspark.virtualenv.enabled\": \"true\", \"spark.pyspark.virtualenv.type\": \"native\", \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\", \"spark.sql.files.ignoreCorruptFiles\": \"true\", \"spark.dynamicAllocation.executorIdleTimeout\": \"18000\", \"spark.driver.memory\": \"12g\", \"spark.executor.memory\": \"12g\", \"spark.driver.cores\": \"3\", \"spark.driver.maxResultSize\": \"10g\", \"spark.yarn.executor.Overhead\": \"10g\", \"livy.server.session.timeout\": \"10h\"}, \"proxyUser\": \"assumed-role_fdp_blitvin-Isengard\"}\n",
      "\n",
      "[I 2023-03-20 15:25:02,414.414 configure_magic] Sending request to update kernel. Please wait while the kernel will be refreshed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel is successfully refreshed."
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "     \"spark.pyspark.python\": \"python3\"\n",
    "    ,\"spark.pyspark.virtualenv.enabled\": \"true\"\n",
    "    ,\"spark.kubernetes.executor.node.selector.node-lifecycle\":\"spot\"\n",
    "    ,\"spark.pyspark.virtualenv.type\":\"native\"\n",
    "    ,\"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    ,\"spark.sql.files.ignoreCorruptFiles\":\"true\"\n",
    "    ,\"spark.dynamicAllocation.executorIdleTimeout\":\"18000\"\n",
    "    ,\"spark.driver.memory\":\"32g\"\n",
    "    ,\"spark.driver.cores\":\"32\"\n",
    "    ,\"spark.driver.maxResultSize\":\"24g\"\n",
    "    ,\"spark.executor.memory\":\"32g\"\n",
    "    ,\"spark.network.timeout\":\"300\"\n",
    "    ,\"spark.executor.cores\":\"6\"\n",
    "    ,\"spark.yarn.executor.Overhead\":\"12g\"\n",
    "    ,\"spark.dynamicAllocation.maxExecutors\":\"500\"\n",
    "    ,\"livy.server.session.timeout\":\"24h\"\n",
    "    ,\"spark.sql.shuffle.partitions\":\"15000\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#    ,\"spark.sql.adaptive.enabled\":\"true\"\n",
    "#    ,\"spark.sql.adaptive.coalescePartitions.enabled\":\"true\"\n",
    "#    ,\"spark.sql.adaptive.coalescePartitions.initialPartitionNum\":\"1000\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:30:55.015888Z",
     "iopub.status.busy": "2023-03-20T15:30:55.015459Z",
     "iopub.status.idle": "2023-03-20T15:30:57.401808Z",
     "shell.execute_reply": "2023-03-20T15:30:57.401238Z",
     "shell.execute_reply.started": "2023-03-20T15:30:55.015800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp==3.8.1\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib64/python3.7/site-packages (from aiohttp==3.8.1) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib64/python3.7/site-packages (from aiohttp==3.8.1) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib64/python3.7/site-packages (from aiohttp==3.8.1) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from aiohttp==3.8.1) (0.13.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp==3.8.1) (3.4)\n",
      "Installing collected packages: aiohttp\n",
      "Successfully installed aiohttp-3.8.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "awswrangler 2.19.0 requires pyarrow<10.1.0,>=2.0.0, but you'll have pyarrow 11.0.0 which is incompatible.\n",
      "aiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you'll have botocore 1.29.69 which is incompatible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import s3fs\n",
    "import fsspec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as py_f\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:30:57.403172Z",
     "iopub.status.busy": "2023-03-20T15:30:57.402886Z",
     "iopub.status.idle": "2023-03-20T15:30:57.457413Z",
     "shell.execute_reply": "2023-03-20T15:30:57.456860Z",
     "shell.execute_reply.started": "2023-03-20T15:30:57.403151Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0-amzn-1 \n",
      "\n",
      "\n",
      "('spark.kubernetes.executor.pod.allowlistFile', '/etc/spark/conf/executor-pod-template-pod-allowlist.txt')\n",
      "('spark.eventLog.enabled', 'true')\n",
      "('spark.pyspark.virtualenv.packages', 'aiohttp==3.8.1')\n",
      "('spark.eventLog.dir', 'file:///var/log/spark/apps')\n",
      "('spark.kubernetes.memoryOverheadFactor', '0.4')\n",
      "('spark.kubernetes.executor.podTemplateContainerName', 'spark-kubernetes-executor')\n",
      "('spark.app.id', 'spark-53fab8b52ed84ee5b8e16ffc516efe61')\n",
      "('spark.kubernetes.driverEnv.HTTP2_DISABLE', 'true')\n",
      "('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter')\n",
      "('spark.kubernetes.executor.selector.node.role', 'managed-endpoint-1-notebook')\n",
      "('spark.blacklist.decommissioning.timeout', '1h')\n",
      "('spark.kubernetes.driver.node.selector.node-lifecycle', 'on-demand')\n",
      "('spark.app.name', 'k24da05fc-f56d-4d6d-87d1-e71f96ef114a')\n",
      "('spark.hadoop.dynamodb.customAWSCredentialsProvider', 'com.amazonaws.auth.WebIdentityTokenCredentialsProvider')\n",
      "('spark.kubernetes.driver.container.allowlistFile', '/etc/spark/conf/driver-pod-template-container-allowlist.txt')\n",
      "('spark.driver.cores', '3')\n",
      "('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions')\n",
      "('spark.dynamicAllocation.executorAllocationRatio', '1')\n",
      "('spark.driver.maxResultSize', '10g')\n",
      "('spark.kubernetes.driver.podTemplateContainerName', 'spark-kubernetes-driver')\n",
      "('spark.history.fs.logDirectory', 'file:///var/log/spark/apps')\n",
      "('spark.kubernetes.executor.label.kernel_id', '24da05fc-f56d-4d6d-87d1-e71f96ef114a')\n",
      "('spark.kubernetes.executor.node.selector.spark-role', 'executor')\n",
      "('spark.yarn.heterogeneousExecutors.enabled', 'false')\n",
      "('spark.driver.extraLibraryPath', '/etc/hadoop/conf:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.pyspark.python', 'python3')\n",
      "('spark.yarn.executor.Overhead', '10g')\n",
      "('spark.kubernetes.container.image.pullPolicy', 'Always')\n",
      "('spark.kubernetes.submitInDriver', 'true')\n",
      "('spark.kubernetes.driver.node.selector.spark-role', 'driver')\n",
      "('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70\")\n",
      "('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.executor.extraClassPath', '/etc/hadoop/conf:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.kubernetes.executor.podNamePrefix', 'k24da05fc-f56d-4d6d-87d1-e71f96ef114a-2e4ee286ffa0c0f7')\n",
      "('spark.driver.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70\")\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.kubernetes.driver.podTemplateValidation.enabled', 'true')\n",
      "('spark.hadoop.hive.metastore.client.factory.class', 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory')\n",
      "('spark.kubernetes.allocation.batch.size', '5')\n",
      "('spark.driver.port', '7078')\n",
      "('spark.executorEnv.LOG_CONTEXT_WITH_PATH_SEPARATOR', '')\n",
      "('spark.decommissioning.timeout.threshold', '20')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true')\n",
      "('spark.kubernetes.driver.pod.allowlistFile', '/etc/spark/conf/driver-pod-template-pod-allowlist.txt')\n",
      "('spark.pyspark.virtualenv.enabled', 'true')\n",
      "('spark.kubernetes.driver.label.component', 'kernel')\n",
      "('spark.dynamicAllocation.shuffleTracking.timeout', '300s')\n",
      "('spark.dynamicAllocation.executorIdleTimeout', '18000')\n",
      "('spark.sql.files.ignoreCorruptFiles', 'true')\n",
      "('spark.kubernetes.executor.label.app', 'enterprise-gateway')\n",
      "('spark.pyspark.virtualenv.bin.path', '/usr/bin/virtualenv')\n",
      "('spark.kubernetes.driver.container.image', '614393260192.dkr.ecr.us-east-2.amazonaws.com/cdk-hnb659fds-container-assets-614393260192-us-east-2:754519b19b5f65eae94d49cadef904b39b505c739ff4f9aae8cafa83e51af8b9')\n",
      "('spark.kubernetes.client.dependency.propagation', 'false')\n",
      "('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000')\n",
      "('spark.authenticate', 'true')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2')\n",
      "('spark.dynamicAllocation.maxExecutors', '50')\n",
      "('spark.shuffle.service.enabled', 'false')\n",
      "('spark.kubernetes.executor.podTemplateFile', '/opt/spark/pod-template/pod-spec-template.yml')\n",
      "('spark.kubernetes.driver.request.cores', '0.5')\n",
      "('spark.driver.memory', '12g')\n",
      "('spark.kubernetes.driver.pod.name', 'k24da05fc-f56d-4d6d-87d1-e71f96ef114a-50f71686ff9fd0b0-driver')\n",
      "('spark.pyspark.virtualenv.type', 'native')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.kubernetes.executor.podTemplateValidation.enabled', 'true')\n",
      "('spark.hadoop.fs.defaultFS', 'file:///')\n",
      "('spark.app.startTime', '1679325969933')\n",
      "('spark.app.submitTime', '1679325968199')\n",
      "('spark.executor.cores', '4')\n",
      "('spark.driver.bindAddress', '10.0.96.172')\n",
      "('spark.kubernetes.executor.node.selector.node-lifecycle', 'spot')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.kubernetes.submission.waitAppCompletion', 'false')\n",
      "('spark.kubernetes.executor.label.emr-containers.amazonaws.com/kernel-type', 'PySpark')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.master', 'k8s://https://172.20.0.1:443')\n",
      "('spark.kubernetes.executor.container.allowlistFile', '/etc/spark/conf/executor-pod-template-container-allowlist.txt')\n",
      "('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true')\n",
      "('spark.dynamicAllocation.shuffleTracking.enabled', 'true')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true')\n",
      "('spark.kubernetes.driver.label.kernel_id', '24da05fc-f56d-4d6d-87d1-e71f96ef114a')\n",
      "('spark.kubernetes.namespace', 'adxuseast2emr')\n",
      "('spark.kubernetes.executor.label.component', 'worker')\n",
      "('spark.history.ui.port', '18080')\n",
      "('spark.driver.blockManager.port', '7079')\n",
      "('spark.hadoop.fs.s3.customAWSCredentialsProvider', 'com.amazonaws.auth.WebIdentityTokenCredentialsProvider')\n",
      "('spark.executor.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.driver.extraClassPath', '/etc/hadoop/conf:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/emrfs/conf:/docker/usr/share/aws/emr/emrfs/lib/*:/docker/usr/share/aws/emr/emrfs/auxlib/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.resourceManager.cleanupExpiredHost', 'true')\n",
      "('spark.driver.host', 'spark-8f682986ff9fd465-driver-svc.adxuseast2emr.svc')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'emr-containers-sa-spark-jeg-kernel-614393260192-1290uyahhbialm60icp0xrrxth1kcn9j6a76dmo6zat9i8skpkt4681h0l')\n",
      "('spark.kubernetes.driver.label.app', 'enterprise-gateway')\n",
      "('spark.files.fetchFailure.unRegisterOutputOnHost', 'true')\n",
      "('spark.kubernetes.driver.label.emr-containers.amazonaws.com/kernel-type', 'PySpark')\n",
      "('spark.executor.memory', '12g')\n",
      "('spark.kubernetes.executor.container.image', '614393260192.dkr.ecr.us-east-2.amazonaws.com/cdk-hnb659fds-container-assets-614393260192-us-east-2:754519b19b5f65eae94d49cadef904b39b505c739ff4f9aae8cafa83e51af8b9')\n",
      "('spark.executor.extraLibraryPath', '/etc/hadoop/conf:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.eventLog.logBlockUpdates.enabled', 'true')\n",
      "('spark.kubernetes.resource.type', 'python')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.kubernetes.driver.selector.node.role', 'managed-endpoint-1-notebook')\n",
      "('spark.dynamicAllocation.minExecutors', '0')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.kubernetes.authenticate.executor.serviceAccountName', 'emr-containers-sa-spark-jeg-kernel-614393260192-1290uyahhbialm60icp0xrrxth1kcn9j6a76dmo6zat9i8skpkt4681h0l')\n",
      "('spark.kubernetes.executor.request.cores', '3.5')\n",
      "('spark.blacklist.decommissioning.enabled', 'true')\n"
     ]
    }
   ],
   "source": [
    "print(spark.version,\"\\n\\n\")\n",
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for conf in configurations:\n",
    "    print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class MtRoundLot():\n",
    "        def __init__(self,part_experiment_id, is_debug,debug_symbol='AMZN'):\n",
    "            self.s3_dir_root=\"s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0\"\n",
    "            self.s3_dir_root_prepped=\"s3://maystreetdata/feeds_norm/partition_scheme_experiments_7/mstnorm_parquet_0_5_0/\"\n",
    "            self.s3_dir_partition_experiments =f\"s3://maystreetdata/feeds_norm/partition_scheme_experiments_{part_experiment_id}/mstnorm_parquet_0_5_0\"\n",
    "            self.s3_dir_root_results =f\"s3://maystreetdata/analysis/\"\n",
    "            self.tables={\"mt_roundlot_bbo\":{\"tables\":[f\"{self.s3_dir_root}/mt=bbo_quote/\"]},\n",
    "                         \"mt_roundlot_nbbo\":{\"tables\":[f\"{self.s3_dir_root}/mt=nbbo_quote/\"]},\n",
    "                         \"mt_oddlot\":{\"tables\":[f\"{self.s3_dir_root}/mt=aggregated_price_update/\"]},\n",
    "                         \"mt_trade\":{\"tables\":[f\"{self.s3_dir_root}/mt=trade/\"]}\n",
    "                        }\n",
    "            self.tables_prepped={\"mt_roundlot_bbo\":{\"tables\":[f\"{self.s3_dir_root_prepped}/mt_roundlot_bbo.parquet\"]},\n",
    "                         \"mt_roundlot_nbbo\":{\"tables\":[f\"{self.s3_dir_root_prepped}/mt_roundlot_nbbo.parquet\"]},\n",
    "                         \"mt_oddlot\":{\"tables\":[f\"{self.s3_dir_root_prepped}/mt_oddlot.parquet\"]}\n",
    "                        }\n",
    "            self.raw_df ={}\n",
    "            self.raw_df_prepped ={}\n",
    "            self.stats_df ={}\n",
    "            self.data_validation ={}\n",
    "            self.column_map={}\n",
    "            self.joined_df={}\n",
    "            self.column_map['mt_oddlot']={\n",
    "                'ask':'AskPrice_1'\n",
    "                ,'bid':'BidPrice_1'\n",
    "                ,'timestamp':'LastExchangeTimestamp'\n",
    "                ,'seq_number':'LastSequenceNumber'\n",
    "                ,'BidQuantity':'BidQuantity_1'\n",
    "                ,'AskQuantity':'AskQuantity_1'\n",
    "                ,'partition_by':['Product','FeedType',\"Feed\",\"dt\",'f','MarketParticipant','is_trading_hours','hour_est']\n",
    "            }\n",
    "            self.column_map['mt_roundlot_bbo']={\n",
    "                'ask':'AskPrice'\n",
    "                ,'bid':'BidPrice'\n",
    "                ,'timestamp':'ExchangeTimestamp'\n",
    "                ,'seq_number':'SequenceNumber'\n",
    "                ,'BidQuantity':'BidQuantity'\n",
    "                ,'AskQuantity':'AskQuantity'\n",
    "                ,'partition_by':['Product','FeedType',\"Feed\",\"dt\",'f','is_trading_hours','hour_est']\n",
    "            }\n",
    "            self.column_map['mt_roundlot_nbbo']={\n",
    "                'ask':'AskPrice'\n",
    "                ,'bid':'BidPrice'\n",
    "                ,'timestamp':'ExchangeTimestamp'\n",
    "                ,'seq_number':'SequenceNumber'\n",
    "                ,'BidQuantity':'BidQuantity'\n",
    "                ,'AskQuantity':'AskQuantity'\n",
    "                ,'partition_by':['Product','FeedType',\"Feed\",\"dt\",'f','is_trading_hours','hour_est']\n",
    "            }\n",
    "            self.round_factor=0.333333333333\n",
    "            self.is_debug=is_debug\n",
    "            self.debug_symbol=debug_symbol\n",
    "            print(self.s3_dir_partition_experiments)\n",
    "        def set_data_prepped(self,data_label):\n",
    "            col_map=self.column_map.get(data_label)\n",
    "            data_files = self.tables_prepped.get(data_label).get('tables')\n",
    "            data_df=None\n",
    "            for one_file in data_files:\n",
    "                if self.is_debug:\n",
    "                    one_data_df=spark.read.parquet(one_file).where(f\"Product=='{self.debug_symbol}'\")\n",
    "                else:\n",
    "                    one_data_df=spark.read.parquet(one_file)\n",
    "                if data_df is None:\n",
    "                    data_df=one_data_df\n",
    "                else:\n",
    "                    data_df=data_df.union(one_data_df)\n",
    "            self.raw_df_prepped[f\"{data_label}\"]=data_df\n",
    "        def set_data(self,data_label, is_raw=False):\n",
    "            col_map=self.column_map.get(data_label)\n",
    "            data_files = self.tables.get(data_label).get('tables')\n",
    "            data_df=None\n",
    "            for one_file in data_files:\n",
    "                feed_filters = self.tables.get(data_label).get('feeds',None)\n",
    "                path_parts= one_file.split(\"/\")\n",
    "                feed_type=path_parts[len(path_parts)-2:len(path_parts)-1][0]\n",
    "                if feed_filters is not None:\n",
    "                    filter_string='\"'+'\",\"'.join(feed_filters)+'\"'\n",
    "                    one_data_df = spark.read.parquet(one_file).filter(f'Feed in ({filter_string})')\n",
    "                else:\n",
    "                    one_data_df = spark.read.parquet(one_file)\n",
    "                if self.is_debug:\n",
    "                    one_data_df=one_data_df.where(f\"Product=='{self.debug_symbol}'\")\n",
    "                if 'f' not in one_data_df.columns:\n",
    "                    one_data_df = one_data_df.withColumn('f', py_f.col(\"Feed\"))\n",
    "                if data_df is None:\n",
    "                    data_df=one_data_df\n",
    "                else:\n",
    "                    data_df=data_df.union(one_data_df)\n",
    "                data_df = data_df.withColumn('FeedType', py_f.lit(feed_type))\\\n",
    "                .select('FeedType','Feed','f','Product',col_map['bid'],col_map['ask'],col_map['timestamp'],col_map['BidQuantity'],col_map['AskQuantity'])\\\n",
    "                .groupBy('FeedType','Feed','f','Product',col_map['timestamp']).agg(\n",
    "                     py_f.round(py_f.max(col_map['bid']),3).alias(f'best_bid_{data_label}')\n",
    "                    ,py_f.round(py_f.min(col_map['ask']),3).alias(f'best_ask_{data_label}')\n",
    "                    ,py_f.round(py_f.max(col_map['BidQuantity']),3).alias(f'bid_quantity_{data_label}')\n",
    "                    ,py_f.round(py_f.max(col_map['AskQuantity']),3).alias(f'ask_quantity_{data_label}')\n",
    "                ).withColumnRenamed('FeedType', f'FeedType_{data_label}')\\\n",
    "                .withColumnRenamed('Feed', f'Feed_{data_label}')\\\n",
    "                .withColumnRenamed(col_map['timestamp'], f'exchange_timestamp_{data_label}')\\\n",
    "                .withColumnRenamed('f', f'f_{data_label}')\\\n",
    "                .withColumn(f\"mid_{data_label}\",(py_f.col(f'best_ask_{data_label}')+py_f.col(f'best_bid_{data_label}'))/py_f.lit(2))\\\n",
    "                .withColumn(f\"bid_ask_{data_label}\",(py_f.col(f'best_ask_{data_label}')-py_f.col(f'best_bid_{data_label}'))/py_f.col(f\"mid_{data_label}\")) \\\n",
    "                .withColumn(f'timestamp_ts_utc_{data_label}',py_f.from_unixtime(py_f.col(f'exchange_timestamp_{data_label}')/1000/1000/1000))\\\n",
    "                .withColumn(f'timestamp_ts_est_{data_label}',py_f.from_utc_timestamp((py_f.from_unixtime(py_f.col(f'exchange_timestamp_{data_label}')/1000/1000/1000)),'America/New_York'))\\\n",
    "                .withColumn(f'time_ts_est_{data_label}',py_f.date_format(f'timestamp_ts_est_{data_label}', 'HH:mm:ss'))\\\n",
    "                .withColumn(f'hour_est_{data_label}',py_f.date_format(f'timestamp_ts_est_{data_label}', 'HH'))\\\n",
    "                .withColumn(f'date_est_{data_label}',py_f.date_format(f'timestamp_ts_est_{data_label}', 'yyyy-MM-dd'))\\\n",
    "                .withColumn(f'is_trading_hours', ((py_f.col(f'timestamp_ts_est_{data_label}')>=py_f.lit('09:30:00'))&(py_f.col(f'timestamp_ts_est_{data_label}')<=py_f.lit('15:59:00'))))\n",
    "\n",
    "            part_by = [f'FeedType_{data_label}',\"Product\"]\n",
    "            self.raw_df[f\"{data_label}\"]=data_df.repartition(15000,*part_by)\n",
    "        def dv_universe(self):\n",
    "            dv_key='universe_check'\n",
    "            self.data_validation[dv_key]={}\n",
    "            for one_key in self.raw_df.keys():\n",
    "                one_df = self.raw_df.get(one_key)\n",
    "                col_name = f\"{one_key}_ticker_count\"\n",
    "                curr_count = one_df.agg(py_f.countDistinct(\"Product\").alias(col_name)).collect()\n",
    "                curr_count =    [i.__getitem__(col_name) for i in curr_count][0]\n",
    "                self.data_validation[dv_key][one_key]=curr_count\n",
    "\n",
    "        def dv_ts_unique(self):\n",
    "            dv_key='ts_unique_check'\n",
    "            self.data_validation[dv_key]={}\n",
    "            for one_key in self.raw_df.keys():\n",
    "                one_df = self.raw_df.get(one_key)\n",
    "                col_map=self.column_map.get(one_key)\n",
    "                ts_field=col_map.get('timestamp')\n",
    "                seq_field=col_map.get('seq_number')\n",
    "                count_alias,countDistinct_alias = f'count_{ts_field}',f'countDistinct_{ts_field}'\n",
    "                uniq_ts_check = one_df.groupBy(col_map.get('partition_by')).agg(\n",
    "                    py_f.count(ts_field).alias(count_alias),py_f.countDistinct(ts_field,seq_field).alias(countDistinct_alias)\n",
    "                ).where(f'{count_alias}>{countDistinct_alias}').count()\n",
    "                self.data_validation[dv_key][one_key]=uniq_ts_check\n",
    "        def set_volume_ptile(self):\n",
    "            by_prod_feed=self.raw_df_prepped[\"mt_roundlot_nbbo\"].groupBy('Product').count().orderBy('Product')\n",
    "            by_prod_feed=by_prod_feed.select(\"Product\",'count',\n",
    "                py_f.round((py_f.floor(py_f.percent_rank().over( Window.partitionBy().orderBy(by_prod_feed['count']))/py_f.lit(self.round_factor))*py_f.lit(self.round_factor)),1).alias(\"update_count_pctrank\"))\n",
    "            by_prod_feed=by_prod_feed.withColumn('volume_level',py_f.when(py_f.col('update_count_pctrank')==0.0,'low')\\\n",
    "                                                                                    .otherwise(py_f.when(py_f.col('update_count_pctrank')==0.3,'moderate').otherwise('high'))).cache()\n",
    "            #by_prod_feed.groupBy('update_count_pctrank').count()\n",
    "            self.volume_rank_df = by_prod_feed\n",
    "\n",
    "        def set_common_universe(self):\n",
    "            bbo_nbbo_cols = ['Product','Feed','dt','f','bidask_spread_timew_avg','data_count','is_trading_hours','hour_est']\n",
    "            df1=self.stats_df['mt_roundlot_bbo_stats_agg'].select(bbo_nbbo_cols)\\\n",
    "            .withColumnRenamed('bidask_spread_timew_avg',f'bidask_spread_timew_avg_bbo_roundlot').withColumnRenamed('data_count',f'data_count_bbo_roundlot')\n",
    "            df2=self.stats_df['mt_roundlot_nbbo_stats_agg'].select(bbo_nbbo_cols)\\\n",
    "            .withColumnRenamed('bidask_spread_timew_avg',f'bidask_spread_timew_avg_nbbo_roundlot').withColumnRenamed('data_count',f'data_count_nbbo_roundlot')\n",
    "            temp_df = df1.join(df2\n",
    "                     ,(df1.Product==df2.Product)\n",
    "                     & (df1.Feed==df2.Feed)\n",
    "                     & (df1.dt==df2.dt)\n",
    "                     & (df1.f==df2.f)\n",
    "                     & (df1.is_trading_hours==df2.is_trading_hours)\n",
    "                     & (df1.hour_est==df2.hour_est)\n",
    "                    ).drop(df2.Product).drop(df2.Feed).drop(df2.dt).drop(df2.f).drop(df2.is_trading_hours).drop(df2.hour_est)\n",
    "\n",
    "            odd_lot_cols = ['Product','dt','bidask_spread_timew_avg','FeedType','Feed','f','data_count','is_trading_hours','hour_est']\n",
    "            df3 = self.stats_df['mt_oddlot_stats_agg'].select(odd_lot_cols)\\\n",
    "                    .withColumnRenamed('bidask_spread_timew_avg',f'bidask_spread_timew_avg_oddlot')\\\n",
    "                    .withColumnRenamed('data_count',f'data_count_oddlot')\\\n",
    "                    .withColumnRenamed('Feed',f'Feed_oddlot')\\\n",
    "                    .withColumnRenamed('f',f'f_oddlot')\n",
    "            final_df=temp_df.join(df3\n",
    "                     ,(temp_df.Product==df3.Product)\n",
    "                     & (temp_df.dt==df3.dt)\n",
    "                     & (temp_df.is_trading_hours==df3.is_trading_hours)\n",
    "                     & (temp_df.hour_est==df3.hour_est)\n",
    "                                 ).drop(df3.Product).drop(df3.dt).drop(temp_df.is_trading_hours).drop(temp_df.hour_est)\n",
    "            volume_rank_df = self.volume_rank_df.select('Product','update_count_pctrank')\n",
    "            final_df=final_df.join(volume_rank_df\n",
    "                                  ,(volume_rank_df.Product==final_df.Product)).drop(volume_rank_df.Product)\n",
    "            self.stats_df['all_by_symbol_feed_date']=final_df.cache()\n",
    "\n",
    "        def calc_trade_stats(self):\n",
    "            trades_df= spark.read.parquet('s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0/mt=trade/')\n",
    "            trades_df=trades_df.withColumn('is_odd_lot',py_f.when(py_f.col(\"Quantity\")<py_f.lit(100),1).otherwise(0))\\\n",
    "                                .withColumn('notional',py_f.col(\"Price\")*py_f.col(\"Quantity\"))\\\n",
    "                                .withColumn('is_FINRA',py_f.col(\"MarketParticipant\")=='FINRA')\n",
    "\n",
    "\n",
    "            per_item_agg=trades_df.groupBy('Feed','Product','Printable','f','is_odd_lot','is_FINRA').agg(py_f.sum('Quantity').alias(\"share_total\")\n",
    "                                                                                            ,py_f.sum('notional').alias(\"volume_total\")\n",
    "                                                                                            ,py_f.count('notional').alias(\"trade_count_total\")\n",
    "                                                                                             )\n",
    "\n",
    "            per_item_agg_pivot = per_item_agg.groupBy('Feed','Product','Printable','f','is_FINRA').pivot('is_odd_lot').agg(py_f.sum('share_total').alias(\"share_total\")\n",
    "                                                                                                                ,py_f.sum('volume_total').alias(\"volume_total\")\n",
    "                                                                                                                ,py_f.sum('trade_count_total').alias(\"trade_count_total\")\n",
    "                                                                                                               )\n",
    "            per_item_agg_pivot=per_item_agg_pivot.withColumn('share_total',py_f.col(\"0_share_total\")+py_f.col(\"1_share_total\"))\\\n",
    "                                                 .withColumn('volume_total',py_f.col(\"0_volume_total\")+py_f.col(\"1_volume_total\"))\\\n",
    "                                                 .withColumn('trade_count_total',py_f.col(\"0_trade_count_total\")+py_f.col(\"1_trade_count_total\"))\n",
    "            all_us_eq=per_item_agg_pivot.where(\"(Feed=='CTS' or Feed=='UTDF') and Printable=='Printable'\").groupBy('Printable').agg(py_f.sum(\"0_share_total\").alias(\"0_share_total\")\n",
    "                                               ,py_f.sum(\"0_volume_total\").alias(\"0_volume_total\")\n",
    "                                               ,py_f.sum(\"1_share_total\").alias(\"1_share_total\")\n",
    "                                               ,py_f.sum(\"1_volume_total\").alias(\"1_volume_total\")\n",
    "                                               ,py_f.sum(\"share_total\").alias(\"share_total\")\n",
    "                                               ,py_f.sum(\"volume_total\").alias(\"volume_total\")\n",
    "                                               ,((py_f.sum(\"1_share_total\")/py_f.sum(\"share_total\"))*py_f.lit(100)).alias('ol_pct_share')\n",
    "                                               ,((py_f.sum(\"1_volume_total\")/py_f.sum(\"volume_total\"))*py_f.lit(100)).alias('ol_pct_volume')\n",
    "                                               ,((py_f.sum(\"1_trade_count_total\")/py_f.sum(\"trade_count_total\"))*py_f.lit(100)).alias('ol_pct_trade_count')\n",
    "                                              ).toPandas()\n",
    "            all_us_eq['label']=\"all US equity securities\"\n",
    "            all_us_eq_DARK_POOL_ONLY=per_item_agg_pivot.where(\"(Feed=='CTS' or Feed=='UTDF') and Printable=='Printable' and is_FINRA==true\").groupBy('Printable').agg(py_f.sum(\"0_share_total\").alias(\"0_share_total\")\n",
    "                                                       ,py_f.sum(\"0_volume_total\").alias(\"0_volume_total\")\n",
    "                                                       ,py_f.sum(\"1_share_total\").alias(\"1_share_total\")\n",
    "                                                       ,py_f.sum(\"1_volume_total\").alias(\"1_volume_total\")\n",
    "                                                       ,py_f.sum(\"share_total\").alias(\"share_total\")\n",
    "                                                       ,py_f.sum(\"volume_total\").alias(\"volume_total\")\n",
    "                                                       ,((py_f.sum(\"1_share_total\")/py_f.sum(\"share_total\"))*py_f.lit(100)).alias('ol_pct_share')\n",
    "                                                       ,((py_f.sum(\"1_volume_total\")/py_f.sum(\"volume_total\"))*py_f.lit(100)).alias('ol_pct_volume')\n",
    "                                                       ,((py_f.sum(\"1_trade_count_total\")/py_f.sum(\"trade_count_total\"))*py_f.lit(100)).alias('ol_pct_trade_count')\n",
    "                                                      ).toPandas()\n",
    "            all_us_eq_DARK_POOL_ONLY['label']=\"all US equity on-exchange\"\n",
    "\n",
    "            all_us_eq_DARK_POOL_EXCL=per_item_agg_pivot.where(\"(Feed=='CTS' or Feed=='UTDF') and Printable=='Printable' and is_FINRA==false\").groupBy('Printable').agg(py_f.sum(\"0_share_total\").alias(\"0_share_total\")\n",
    "                                                       ,py_f.sum(\"0_volume_total\").alias(\"0_volume_total\")\n",
    "                                                       ,py_f.sum(\"1_share_total\").alias(\"1_share_total\")\n",
    "                                                       ,py_f.sum(\"1_volume_total\").alias(\"1_volume_total\")\n",
    "                                                       ,py_f.sum(\"share_total\").alias(\"share_total\")\n",
    "                                                       ,py_f.sum(\"volume_total\").alias(\"volume_total\")\n",
    "                                                       ,((py_f.sum(\"0_share_total\")/py_f.sum(\"share_total\"))*py_f.lit(100)).alias('ol_pct_share')\n",
    "                                                       ,((py_f.sum(\"0_volume_total\")/py_f.sum(\"volume_total\"))*py_f.lit(100)).alias('ol_pct_volume')\n",
    "                                                       ,((py_f.sum(\"0_trade_count_total\")/py_f.sum(\"trade_count_total\"))*py_f.lit(100)).alias('ol_pct_trade_count')\n",
    "                                                      ).toPandas()\n",
    "            all_us_eq_DARK_POOL_EXCL['label']=\"all US equity off-exchange (dark pool)\"\n",
    "            self.raw_df['trades_df']=trades_df\n",
    "            self.stats_df['1_trade_per_item_agg_pivot']=per_item_agg_pivot\n",
    "            self.stats_df['1_trade_stats_all_us_eq']=all_us_eq\n",
    "            self.stats_df['1_trade_stats_all_us_eq_DARK_POOL_ONLY']=all_us_eq_DARK_POOL_ONLY\n",
    "            self.stats_df['1_trade_stats_all_us_eq_DARK_POOL_EXCL']=all_us_eq_DARK_POOL_EXCL\n",
    "            display_cols = [\"label\",\"ol_pct_share\",\"ol_pct_volume\",\"ol_pct_trade_count\"]\n",
    "            rename_cols  = [\"label\",\"Oddlot shares trading(%)\",\"Oddlot $ volume(%)\",\"Oddlot trade count(%)\"]\n",
    "            disp_pd_array=[]\n",
    "            for one_disp_item in ['1_trade_stats_all_us_eq','1_trade_stats_all_us_eq_DARK_POOL_ONLY','1_trade_stats_all_us_eq_DARK_POOL_EXCL']:\n",
    "                disp_pd_array.append(self.stats_df[one_disp_item][display_cols])\n",
    "            disp_pd = pd.concat(disp_pd_array).set_index('label')\n",
    "            disp_pd=disp_pd.rename(columns=dict(zip(display_cols,rename_cols)))\n",
    "            self.stats_df['1_display_all']=disp_pd\n",
    "        def join_dfs(self,dl1,dl2):\n",
    "            df1=self.raw_df_prepped[dl1]\n",
    "            df2=self.raw_df_prepped[dl2]\n",
    "            df1_df2=df1.join(df2, (df1.Product==df2.Product) &  (df1[f'exchange_timestamp_{dl1}']==df2[f'exchange_timestamp_{dl2}']),'inner').drop(df2.Product)\n",
    "            df1_df2=df1_df2.withColumn('exchange_timestamp',py_f.when(py_f.col(f'exchange_timestamp_{dl1}').isNull(), py_f.col(f'exchange_timestamp_{dl2}')).otherwise(py_f.col(f'exchange_timestamp_{dl1}')))\n",
    "            self.joined_df[f\"{dl1}_{dl2}\"]=df1_df2.cache()\n",
    "        [\n",
    "        'FeedType_mt_oddlot', 'Feed_mt_oddlot', 'f_mt_oddlot', 'Product'\n",
    "        , 'exchange_timestamp_mt_oddlot', 'best_bid_mt_oddlot', 'best_ask_mt_oddlot', 'bid_ask_mt_oddlot'\n",
    "        , 'timestamp_ts_utc_mt_oddlot', 'timestamp_ts_est_mt_oddlot'\n",
    "        , 'FeedType_mt_roundlot_bbo', 'Feed_mt_roundlot_bbo', 'f_mt_roundlot_bbo'\n",
    "        , 'exchange_timestamp_mt_roundlot_bbo', 'best_bid_mt_roundlot_bbo', 'best_ask_mt_roundlot_bbo', 'bid_ask_mt_roundlot_bbo'\n",
    "        , 'timestamp_ts_utc_mt_roundlot_bbo', 'timestamp_ts_est_mt_roundlot_bbo', 'exchange_timestamp'\n",
    "        ]\n",
    "        def calc_timew_spread_paired(self,dl1,dl2):\n",
    "            part_cols = [f'FeedType_{dl1}', f'Feed_{dl1}', f'f_{dl1}', 'Product', f'FeedType_{dl2}', f'Feed_{dl2}', f'f_{dl2}']\n",
    "            joined_df=self.joined_df[f\"{dl1}_{dl2}\"]\n",
    "            joined_df=joined_df.withColumn('time_est', py_f.date_format(f'timestamp_ts_est_{dl1}', 'HH:mm:ss'))\\\n",
    "                        .withColumn('hour_est', py_f.date_format(f'timestamp_ts_est_{dl1}', 'HH'))\\\n",
    "                        .withColumn('is_trading_hours', ((py_f.col('time_est')>=py_f.lit('09:30:00')) & (py_f.col('time_est')<=py_f.lit('15:59:00'))))\\\n",
    "                        .withColumn('avg_price', (py_f.col(f'best_bid_{dl1}')\n",
    "                                                  +py_f.col(f'best_ask_{dl1}')\n",
    "                                                  +py_f.col(f'best_bid_{dl2}')\n",
    "                                                  +py_f.col(f'best_ask_{dl2}'))/4)\\\n",
    "                        .withColumn('price_bucket', py_f.when((py_f.col('avg_price')>=0) & (py_f.col('avg_price')<250),'00000_00250').otherwise(\\\n",
    "                py_f.when((py_f.col('avg_price')>=250) & (py_f.col('avg_price')<1000),'00250_01000').otherwise(\\\n",
    "                py_f.when((py_f.col('avg_price')>=1000) & (py_f.col('avg_price')<10000),'01000_10000').otherwise('10000_99999'))\n",
    "                ))\n",
    "\n",
    "            prev_window = Window.partitionBy(*[part_cols]).orderBy(py_f.col('exchange_timestamp'))\n",
    "            joined_df = joined_df.withColumn(\"prev_exchange_timestamp\", py_f.lag(py_f.col('exchange_timestamp')).over(prev_window))\n",
    "            joined_df = joined_df.withColumn(\"diff_exchange_timestamp\",joined_df.exchange_timestamp-joined_df.prev_exchange_timestamp)\n",
    "            joined_df = joined_df.withColumn(f\"bidask_timeweight_{dl1}\",joined_df[f'bid_ask_{dl1}']*joined_df.diff_exchange_timestamp)\n",
    "            joined_df = joined_df.withColumn(f\"bidask_timeweight_{dl2}\",joined_df[f'bid_ask_{dl2}']*joined_df.diff_exchange_timestamp)\n",
    "            #df_vol_rank=self.volume_rank_df.drop('update_count_pctrank').drop('count')\n",
    "            #joined_df = df_vol_rank.join(py_f.broadcast(joined_df),(df_vol_rank.Product==joined_df.Product)).drop(df_vol_rank.Product)\n",
    "            self.stats_df[f'joined_df_{dl1}_{dl2}']=joined_df\n",
    "            joined_df_stats_by_symbol=joined_df.groupBy('Product',f'Feed_{dl2}',f'f_{dl2}','is_trading_hours','hour_est','price_bucket').agg(\n",
    "                 py_f.sum(py_f.col('diff_exchange_timestamp')).alias('diff_exchange_timestamp_sum')\n",
    "                ,py_f.sum(py_f.col(f'bidask_timeweight_{dl1}')).alias(f'bidask_timeweight_{dl1}_sum')\n",
    "                ,py_f.sum(py_f.col(f'bidask_timeweight_{dl2}')).alias(f'bidask_timeweight_{dl2}_sum')\n",
    "            ).withColumn(f'bid_ask_tw_{dl1}',py_f.col(f'bidask_timeweight_{dl1}_sum')/py_f.col('diff_exchange_timestamp_sum'))\\\n",
    "            .withColumn(f'bid_ask_tw_{dl2}',py_f.col(f'bidask_timeweight_{dl2}_sum')/py_f.col('diff_exchange_timestamp_sum'))\\\n",
    "            .orderBy('Product',f'Feed_{dl2}',f'f_{dl2}','is_trading_hours').cache()\n",
    "            self.stats_df[f'joined_df_stats_by_symbol_{dl1}_{dl2}']=joined_df_stats_by_symbol\n",
    "\n",
    "            joined_df_stats_by_trading_hour=joined_df_stats_by_symbol.groupBy(f'Feed_{dl2}',f'f_{dl2}','is_trading_hours','hour_est')\\\n",
    "            .agg(\n",
    "                 py_f.mean(py_f.col(f'bid_ask_tw_{dl1}'))\n",
    "                ,py_f.mean(py_f.col(f'bid_ask_tw_{dl2}'))\n",
    "                ,py_f.count(py_f.col(f'bid_ask_tw_{dl1}'))\n",
    "                ,py_f.count(py_f.col(f'bid_ask_tw_{dl2}'))\n",
    "            )\n",
    "            self.stats_df[f'joined_df_stats_by_trading_hour_{dl1}_{dl2}']=joined_df_stats_by_trading_hour\n",
    "\n",
    "            self.stats_df[f'2_spread_by_price_bucket_{dl1}_{dl2}']=self.stats_df[f'joined_df_stats_by_symbol_{dl1}_{dl2}']\\\n",
    "                            .where(\"is_trading_hours==True\").groupBy('price_bucket')\\\n",
    "                            .agg(py_f.mean(f'bid_ask_tw_{dl1}')\\\n",
    "                            .alias(f'spread_mean_{dl1}'),py_f.mean(f'bid_ask_tw_{dl2}').alias(f'spread_mean_{dl2}'))\n",
    "\n",
    "        def add_price_bucket_bbo(self):\n",
    "            roundlot_bbo=self.raw_df.get('mt_roundlot_bbo')\n",
    "            roundlot_bbo =roundlot_bbo.withColumn('avg_price', (py_f.col(f'best_bid_mt_roundlot_bbo')+py_f.col(f'best_ask_mt_roundlot_bbo'))/2)\\\n",
    "                                .withColumn('price_bucket', py_f.when((py_f.col('avg_price')>=0) & (py_f.col('avg_price')<250),'00000_00250').otherwise(\\\n",
    "                        py_f.when((py_f.col('avg_price')>=250) & (py_f.col('avg_price')<1000),'00250_01000').otherwise(\\\n",
    "                        py_f.when((py_f.col('avg_price')>=1000) & (py_f.col('avg_price')<10000),'01000_10000').otherwise('10000_99999'))\n",
    "                        ))\n",
    "            self.raw_df['mt_roundlot_bbo']=roundlot_bbo\n",
    "\n",
    "        def calc_AMZN_before_after_split(self):\n",
    "\n",
    "            start_date = '2022-06-01'\n",
    "            split_date = '2022-06-06'\n",
    "            end_date = '2022-06-08'\n",
    "\n",
    "            roundlot_bbo=self.raw_df.get('mt_roundlot_bbo')\\\n",
    "            .where(\"Product=='AMZN'\")\\\n",
    "            .where(f\"date_est_mt_roundlot_bbo >= '{start_date}'\")\\\n",
    "            .where(f\"date_est_mt_roundlot_bbo <= '{end_date}'\")\\\n",
    "            .withColumn('is_odd_lot',py_f.when((py_f.col('bid_quantity_mt_roundlot_bbo')<=100) | (py_f.col('ask_quantity_mt_roundlot_bbo')<=100),'odd lot').otherwise('round lot'))\n",
    "            roundlot_stats_pd=roundlot_bbo.groupBy('price_bucket','is_odd_lot','date_est_mt_roundlot_bbo').agg(py_f.count('exchange_timestamp_mt_roundlot_bbo').alias(\"message_count\")).toPandas()\n",
    "            roundlot_stats_pivot=pd.pivot_table(roundlot_stats_pd,values=\"message_count\",columns=[\"is_odd_lot\"],index=[\"date_est_mt_roundlot_bbo\"])\n",
    "            roundlot_stats_pivot['total']=roundlot_stats_pivot.sum(axis=1)\n",
    "            roundlot_stats_pivot['oddlot_pct']=roundlot_stats_pivot['odd lot']/roundlot_stats_pivot['total']\n",
    "            self.stats_df[f\"3A_AMZN_split\"]=roundlot_stats_pivot\n",
    "\n",
    "        def calc_oddlot_percent_per_price_bucket(self):\n",
    "            roundlot_bbo=self.raw_df.get('mt_roundlot_bbo')\n",
    "            roundlot_bbo=roundlot_bbo.withColumn('is_odd_lot',py_f.when((py_f.col('bid_quantity_mt_roundlot_bbo')<=100) | (py_f.col('ask_quantity_mt_roundlot_bbo')<=100),'odd lot').otherwise('round lot'))\n",
    "            roundlot_stats = roundlot_bbo.groupBy('price_bucket','is_odd_lot').agg(py_f.count('is_odd_lot').alias('message_count'))\n",
    "            roundlot_stats_pd = roundlot_stats.toPandas()\n",
    "            odd_vs_roundlot_pct=pd.pivot_table(roundlot_stats_pd, values=\"message_count\",index=\"price_bucket\",columns=\"is_odd_lot\")\n",
    "            odd_vs_roundlot_pct=pd.pivot_table(roundlot_stats_pd, values=\"message_count\",index=\"price_bucket\",columns=\"is_odd_lot\")\n",
    "            odd_vs_roundlot_pct['total']=odd_vs_roundlot_pct.sum(axis=1)\n",
    "            odd_vs_roundlot_pct['odd lot pct']=odd_vs_roundlot_pct['odd lot']/odd_vs_roundlot_pct['total']\n",
    "            self.stats_df[f\"3_pct_trading_oddlot_per_price_bucket\"]=odd_vs_roundlot_pct\n",
    "\n",
    "        def calc_timew_spread(self,data_label):\n",
    "            col_map=self.column_map.get(data_label)\n",
    "            l_df =  self.raw_df.get(data_label)\n",
    "            l_df = l_df.withColumn('timestamp_ts_utc',py_f.from_unixtime(py_f.col(col_map.get('timestamp'))/1000/1000/1000))\\\n",
    "                       .withColumn('timestamp_ts_est',py_f.from_utc_timestamp((py_f.from_unixtime(py_f.col(col_map.get('timestamp'))/1000/1000/1000)),'America/New_York'))\\\n",
    "                       .withColumn('time_est', py_f.date_format('timestamp_ts_est', 'HH:mm:ss'))\\\n",
    "                       .withColumn('hour_est', py_f.date_format('timestamp_ts_est', 'HH'))\\\n",
    "                       .withColumn('is_trading_hours', ((py_f.col('time_est')>=py_f.lit('09:30:00'))&(py_f.col('time_est')<=py_f.lit('15:59:00'))))\n",
    "            l_df = l_df.withColumn(\"bid_ask\",(py_f.col(col_map.get('ask'))-py_f.col(col_map.get('bid')))/py_f.col(col_map.get('bid')) )\n",
    "            prev_window = Window.partitionBy(*col_map.get('partition_by')).\\\n",
    "                            orderBy(py_f.col(col_map.get('timestamp')),py_f.col(col_map.get('seq_number')),l_df.bid_ask.desc())\n",
    "            l_df = l_df.withColumn(\"next_LastReceiptTimestamp\", py_f.lead(py_f.col(col_map.get('timestamp'))).over(prev_window))\n",
    "            l_df = l_df.withColumn(\"diff_LastReceiptTimestamp\",py_f.col(col_map.get('timestamp'))-l_df.next_LastReceiptTimestamp)\n",
    "            l_df = l_df.withColumn(\"bidask_timeweight\",l_df.bid_ask*l_df.diff_LastReceiptTimestamp)\n",
    "            bid_ask_agg= l_df.where('diff_LastReceiptTimestamp is not null and bid_ask<100').groupby(*col_map.get('partition_by')).\\\n",
    "                    agg(py_f.sum('diff_LastReceiptTimestamp').alias('time_sum'),\n",
    "                        py_f.sum('bidask_timeweight').alias('bidask_timeweight_sum'),\n",
    "                        py_f.count(py_f.lit(1)).alias('data_count'))\n",
    "            bid_ask_agg=bid_ask_agg.withColumn(\"bidask_spread_timew_avg\",bid_ask_agg.bidask_timeweight_sum/bid_ask_agg.time_sum)\n",
    "            self.stats_df[f\"{data_label}_stats_intermediate\"]=l_df\n",
    "            self.stats_df[f\"{data_label}_stats_agg\"]=bid_ask_agg\n",
    "            self.stats_df[f\"{data_label}_stats_agg_final\"]=bid_ask_agg.agg(py_f.mean(bid_ask_agg.bidask_spread_timew_avg).alias('bidask_mean_timew'),\n",
    "                                                                           py_f.expr('percentile(bidask_spread_timew_avg, array(0.5))').alias('bidask_median_timew'),\n",
    "                                                                            py_f.sum(bid_ask_agg.data_count).alias('data_count'))\n",
    "\n",
    "\n",
    "    def main(exp_label,is_debug,debug_symbol='AMZN'):\n",
    "        mt_roundlot=MtRoundLot(exp_label,is_debug,debug_symbol)\n",
    "        print('mt_oddlot_prepped')\n",
    "        mt_roundlot.set_data_prepped(\"mt_oddlot\")\n",
    "        print('mt_roundlot_bbo_prepped')\n",
    "        mt_roundlot.set_data_prepped(\"mt_roundlot_bbo\")\n",
    "        print('mt_roundlot_nbbo_prepped')\n",
    "        mt_roundlot.set_data_prepped(\"mt_roundlot_nbbo\")\n",
    "        print('mt_oddlot')\n",
    "        mt_roundlot.set_data(\"mt_oddlot\")\n",
    "        print('mt_roundlot_bbo')\n",
    "        mt_roundlot.set_data(\"mt_roundlot_bbo\")\n",
    "        print('mt_roundlot_nbbo')\n",
    "        mt_roundlot.set_data(\"mt_roundlot_nbbo\")\n",
    "        mt_roundlot.join_dfs('mt_oddlot','mt_roundlot_bbo')\n",
    "        mt_roundlot.join_dfs('mt_oddlot','mt_roundlot_nbbo')\n",
    "        mt_roundlot.calc_timew_spread_paired(\"mt_oddlot\",\"mt_roundlot_bbo\")\n",
    "        mt_roundlot.calc_timew_spread_paired(\"mt_oddlot\",\"mt_roundlot_nbbo\")\n",
    "        mt_roundlot.add_price_bucket_bbo()\n",
    "        mt_roundlot.calc_oddlot_percent_per_price_bucket()\n",
    "        mt_roundlot.calc_AMZN_before_after_split()\n",
    "        mt_roundlot.calc_trade_stats()\n",
    "        return(mt_roundlot)\n",
    "mt_roundlot = main(7,False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:30:57.458738Z",
     "iopub.status.busy": "2023-03-20T15:30:57.458333Z",
     "iopub.status.idle": "2023-03-20T15:34:03.305674Z",
     "shell.execute_reply": "2023-03-20T15:34:03.305018Z",
     "shell.execute_reply.started": "2023-03-20T15:30:57.458719Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://maystreetdata/feeds_norm/partition_scheme_experiments_7/mstnorm_parquet_0_5_0\n",
      "mt_oddlot\n",
      "mt_roundlot_bbo\n",
      "mt_roundlot_nbbo\n",
      "mt_oddlot_prepped\n",
      "mt_roundlot_bbo_prepped\n",
      "mt_roundlot_nbbo_prepped\n"
     ]
    }
   ],
   "source": [
    "captions=[\"1.Odd-lot trading in 2022 continues to be a major component of US trading\"\n",
    "          ,\"2.Odd-lot vs. Round-lot quoted spread\"\n",
    "          ,\"3.Higher share prices increase odd-lot trading\"\n",
    "          ,\"3a.AMZN pre/post split on June-6-2022\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-20T16:24:38.867464Z",
     "iopub.status.idle": "2023-03-20T16:24:38.867691Z",
     "shell.execute_reply": "2023-03-20T16:24:38.867585Z",
     "shell.execute_reply.started": "2023-03-20T16:24:38.867574Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mt_roundlot.stats_df[\"1_display_all\"].style.set_caption(f\"Experimentation findings: {captions[0]}\").format({\"Oddlot shares trading(%)\": \"{:20,.0f}%\",\n",
    "                          \"Oddlot $ volume(%)\": \"{:20,.0f}%\",\n",
    "                          \"Oddlot trade count(%)\": \"{:20,.0f}%\"\n",
    "                     }).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '14px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T15:34:03.311578Z",
     "iopub.status.busy": "2023-03-20T15:34:03.311416Z",
     "iopub.status.idle": "2023-03-20T16:24:38.856907Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o594.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.FetchFailedException\n\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1166)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:904)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 3), which maintains the block data to fetch is dead.\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1136)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1128)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1001)\n\t... 25 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1995)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:467)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmt_roundlot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraw_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'mt_roundlot_bbo'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    605\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    606\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 607\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    608\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1322\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1324\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o594.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.FetchFailedException\n\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1166)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:904)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)\n\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 3), which maintains the block data to fetch is dead.\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1136)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1128)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1001)\n\t... 25 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1995)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:467)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "oddlot_bbo=mt_roundlot.stats_df['2_spread_by_price_bucket_mt_oddlot_mt_roundlot_bbo'].toPandas().sort_values(\"price_bucket\")\n",
    "oddlot_bbo['price_improvement']=((oddlot_bbo[\"spread_mean_mt_roundlot_bbo\"]/oddlot_bbo[\"spread_mean_mt_oddlot\"])-1)*1\n",
    "oddlot_bbo.sort_values(\"price_bucket\").style.set_caption(f\"Experimentation findings: {captions[1]}\").format(\n",
    "    {\"spread_mean_mt_oddlot\": \"{:,.2%}\",\n",
    "      \"spread_mean_mt_roundlot_bbo\": \"{:,.2%}\",\n",
    "      \"price_improvement\": \"{:,.2%}\"\n",
    "                     }).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '14px')\n",
    "    ]\n",
    "}]).hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T00:25:52.711961Z",
     "iopub.status.busy": "2022-10-19T00:25:52.711099Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3110b7bad8ce4ef4927f6ce825ce72c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326ae2f55c3744e388a0b031884ed609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oddlot_price_per_bucket=mt_roundlot.stats_df['3_pct_trading_oddlot_per_price_bucket']#.toPandas()#.sort_values(\"price_bucket\")\n",
    "\n",
    "oddlot_price_per_bucket['odd lot pct'].plot.bar()\n",
    "oddlot_price_per_bucket.sort_values(\"price_bucket\").style.set_caption(f\"Experimentation findings: {captions[2]}\").format(\n",
    "    {\"round lot\": \"{:,.2f}\",\n",
    "      \"odd lot\": \"{:,.2f}\",\n",
    "      \"total\": \"{:,.2f}\",\n",
    "      \"odd lot pct\": \"{:,.2%}\"\n",
    "                     }).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '14px')\n",
    "    ]\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "AMZN_pre_post_split=mt_roundlot.stats_df['3A_AMZN_split']#.toPandas()#.sort_values(\"price_bucket\")\n",
    "AMZN_pre_post_split[\"roundlot_pct\"]=1-AMZN_pre_post_split[\"oddlot_pct\"]\n",
    "(AMZN_pre_post_split[['oddlot_pct','roundlot_pct']]*100).plot(kind=\"bar\", ylabel='Percent(%)',stacked=True,figsize=(7,3))\n",
    "AMZN_pre_post_split.style.set_caption(f\"Experimentation findings: {captions[3]}\").format(\n",
    "    {\"round lot\": \"{:,.2f}\",\n",
    "      \"odd lot\": \"{:,.2f}\",\n",
    "      \"total\": \"{:,.2f}\",\n",
    "      \"oddlot_pct\": \"{:,.2%}\",\n",
    "      \"roundlot_pct\": \"{:,.2%}\"\n",
    "                     }).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'blue'),\n",
    "        ('font-size', '14px')\n",
    "    ]\n",
    "}])"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}