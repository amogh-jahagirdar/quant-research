%%configure -f
{ "conf":{
        "spark.pyspark.python": "python3"
    ,"spark.pyspark.virtualenv.enabled": "true"
    ,"spark.pyspark.virtualenv.type":"native"
    ,"spark.pyspark.virtualenv.bin.path":"/usr/bin/virtualenv"
    ,"spark.sql.files.ignoreCorruptFiles":"true"
    ,"spark.dynamicAllocation.executorIdleTimeout":"18000"
    ,"spark.driver.memory":"24g","spark.executor.memory":"24g"
    ,"spark.driver.cores":"5"
    ,"spark.driver.maxResultSize":"5g"
         }
}  

try:
    sc.install_pypi_package("aiohttp==3.8.1")
except: 
    print(f'aiohttp is installed')
try:
    import pandas as pd
except: 
    sc.install_pypi_package("pandas==1.1.5")
    import pandas as pd
try:
    import pyarrow
except: 
    sc.install_pypi_package("pyarrow==0.14.1")
    import pyarrow 
try:
    import s3fs
except: 
    sc.install_pypi_package("s3fs")
    import s3fs 
try:
    import fsspec
except: 
    sc.install_pypi_package("fsspec")
    import fsspec 
if False:
    try:
        import matplotlib
        import matplotlib.pyplot as plt
    except: 
        sc.install_pypi_package("matplotlib")
        import matplotlib
        import matplotlib.pyplot as plt
import pyspark.sql.functions as py_f
from pyspark.sql.window import Window

print(spark.version,"\n\n")
configurations = spark.sparkContext.getConf().getAll()
for conf in configurations:
    print(conf)


one_file="s3://maystreetdata/feeds_norm/partition_scheme_experiments_7/mstnorm_parquet_0_5_0/mt_roundlot_bbo.parquet/"
d_df=spark.read.parquet(one_file)
d_df.show()

f_array=['f=bats_edga',
'f=bats_edgx',
'f=byx',
'f=bzx',
'f=cts_pillar',
'f=iex_deep',
'f=memoir_depth',
'f=miax_pearl_equities_dom',
'f=total_view_bx',
'f=total_view_psx',
'f=total_view',
'f=utdf_binary',
'f=xdp_american_integrated',
'f=xdp_arca_integrated',
'f=xdp_chicago_integrated',
'f=xdp_national_integrated',
'f=xdp_nyse_integrated',]
root_dir = 's3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0/mt=trade/'
may_street_rootdir='s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0'
may_street_feeds=[
'mt=add_order',
'mt=aggregated_price_update',
'mt=auction_summary',
'mt=bbo_quote',
'mt=cancel_order',
'mt=clear_orders',
'mt=feed_status',
'mt=index_update',
'mt=market_status',
'mt=missing_packets',
'mt=modify_order',
'mt=nbbo_quote',
'mt=order_imbalance',
'mt=price_level_update',
'mt=product_announcement',
'mt=product_statistics',
'mt=product_status',
'mt=retail_price_improvement',
'mt=trade_break',
'mt=trade_correction',
'mt=trade',]
f_array=['f=bats_edga/',
'f=bats_edgx/',
'f=byx/',
'f=bzx/',
'f=iex_deep/',
'f=memoir_depth/',
'f=miax_pearl_equities_dom/',
'f=total_view_bx/',
'f=total_view_psx/',
'f=total_view/',
'f=xdp_american_integrated/',
'f=xdp_arca_integrated/',
'f=xdp_chicago_integrated/',
'f=xdp_national_integrated/',
'f=xdp_nyse_integrated/',]
def get_data(root_dir,sub_dir_array):
    all_dfs={}
    data_df=None
    for one_feed in sub_dir_array:
        one_file = f'{root_dir}/{one_feed}/'
        try:
            one_df = spark.read.parquet(one_file)
            all_dfs[one_feed]=one_df
            if data_df is None:
                data_df=one_df
            else:
                data_df=data_df.union(one_df)
        except Exception as l_exc:
             all_dfs[one_feed]=f'exc:{l_exc}'
    return(all_dfs,data_df)
trade_df_array,trade_df=get_data('s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0/mt=aggregated_price_update/',f_array)
#nbbo_df_array,nbbo_df=get_data('s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0/mt=aggregated_price_update',[''])

one_df = spark.read.parquet('s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0/mt=aggregated_price_update/f=bats_edgx/')
one_df.show()

trade_df.groupby('Feed').count().show()

res_dict={}
for k,v in trade_df_array.items():
    res_dict[k] = (v.count())
    
res_dict

trade_df.rdd.getNumPartitions()

l_fname=f"s3://maystreetdata/feeds_norm/partition_scheme_experiments_product_feed_dt/mstnorm_parquet_0_5_0/XDPV2_IEX_Total.parquet"
#trade_df.repartition(1).write\
trade_df.repartition(1000).write\
            .option("header",True) \
            .partitionBy("Product","Feed","dt") \
            .mode("overwrite") \
            .parquet(f"{l_fname}")

                                                          #1636035828600286047
trade_df.where(f"Product=='{'SPY'}' and ExchangeTimestamp<{1661204000000431328}")\
.orderBy('ExchangeTimestamp','SequenceNumber').toPandas().to_csv('s3://maystreetdata/analysis/spy_sample.csv')#.agg(py_f.min(py_f.col('ExchangeTimestamp')),py_f.max(py_f.col('ExchangeTimestamp'))).show()

nbbo_df.where(f"Product=='{'SPY'}' and LastExchangeTimestamp<{1661204000000431328}").show()

spy_nbbo=nbbo_df.where(f"Product=='{'SPY'}'").select('Product','LastExchangeTimestamp','Feed', 'LastSequenceNumber', 'BidPrice_1','BidQuantity_1','AskPrice_1','AskQuantity_1')\
.withColumnRenamed('LastExchangeTimestamp','ExchangeTimestamp_nbbo').withColumnRenamed('Product','Product_nbbo').withColumnRenamed('LastSequenceNumber','SequenceNumber_nbbo')
spy_trade=trade_df.where(f"Product=='{'SPY'}'")

spy_nbbo_trade=spy_nbbo.join(spy_trade,(spy_nbbo.Product_nbbo==spy_trade.Product)  
                             & (spy_nbbo.Feed==spy_trade.Feed) 
                             & (spy_nbbo.ExchangeTimestamp_nbbo==spy_trade.ExchangeTimestamp) ,'outer').drop(spy_trade.Feed)
spy_nbbo_trade=spy_nbbo_trade.withColumn('exchange_ts',py_f.when(py_f.col(f'ExchangeTimestamp_nbbo').isNull(), py_f.col(f'ExchangeTimestamp')).otherwise(py_f.col(f'ExchangeTimestamp_nbbo')))
spy_nbbo_trade.orderBy('exchange_ts','SequenceNumber_nbbo','SequenceNumber', 'ProductSequenceNumber')\
.select('Product_nbbo','SequenceNumber','Feed','SequenceNumber_nbbo', 'exchange_ts','Side', 'BidQuantity_1', 'BidPrice_1','Price','AskPrice_1', 'AskQuantity_1','Quantity','SaleCondition','SaleCondition2','SaleCondition3','SaleCondition4')\
.where("SaleCondition3 !='ExtendedHoursTrade' ").toPandas().to_csv('s3://maystreetdata/analysis/spy_full.csv')

import pyarrow.parquet as pq


class ParquetInspector(object):
    """docstring for ParquetInspector"""

    def __init__(self, parquet_filepath):
        super(ParquetInspector, self).__init__()
        self.parquet_filepath = parquet_filepath

        self.metadata = pq.read_metadata(self.parquet_filepath)

    def footer_size(self):
        return self.metadata_serialized_size()

    def metadata_serialized_size(self):
        return self.metadata.serialized_size

    def row_count(self):
        return self.metadata.num_rows

parquet_path='s3://maystreetdata/feeds_norm/mstnorm_parquet_0_5_0/mt=aggregated_price_update/f=bats_edga/dt=2022-07-20/1.parquet'
inspector = ParquetInspector(parquet_path)
footer_size = inspector.footer_size()


