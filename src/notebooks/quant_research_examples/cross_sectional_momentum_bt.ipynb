{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f0683d-273b-400f-99c5-22a5b251288d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:32:06.259172Z",
     "iopub.status.busy": "2023-01-17T00:32:06.258665Z",
     "iopub.status.idle": "2023-01-17T00:32:21.453274Z",
     "shell.execute_reply": "2023-01-17T00:32:21.453178Z",
     "shell.execute_reply.started": "2023-01-17 00:32:06.265454+00:00"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-01-17 00:32:06,265.265 configure_magic] Magic cell payload received: {\"conf\": {\"spark.pyspark.python\": \"python3\", \"spark.pyspark.virtualenv.enabled\": \"true\", \"spark.pyspark.virtualenv.type\": \"native\", \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\", \"spark.sql.execution.arrow.enabled\": \"true\"}, \"proxyUser\": \"assumed-role_fdp_blitvin-Isengard\"}\n",
      "\n",
      "[I 2023-01-17 00:32:06,265.265 configure_magic] Sending request to update kernel. Please wait while the kernel will be refreshed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel is successfully refreshed."
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "          \"spark.sql.execution.arrow.enabled\":\"true\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab8a1bd-e038-49e6-a7b8-7aa66e90c24e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:34:42.536952Z",
     "iopub.status.busy": "2023-01-17T00:34:42.536455Z",
     "iopub.status.idle": "2023-01-17T00:34:44.783461Z",
     "shell.execute_reply": "2023-01-17T00:34:44.782592Z",
     "shell.execute_reply.started": "2023-01-17T00:34:42.536865Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyEX in /usr/local/lib/python3.7/site-packages (0.5.0)\n",
      "Requirement already satisfied: deprecation>=2.0.6 in /usr/local/lib/python3.7/site-packages (from pyEX) (2.1.0)\n",
      "Requirement already satisfied: pandas>=0.22 in /usr/local/lib64/python3.7/site-packages (from pyEX) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.7/site-packages (from pyEX) (2.28.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from pyEX) (1.16.0)\n",
      "Requirement already satisfied: Pillow>=5.3.0 in /usr/local/lib64/python3.7/site-packages (from pyEX) (9.3.0)\n",
      "Requirement already satisfied: ipython>=7.2.0 in /usr/local/lib/python3.7/site-packages (from pyEX) (7.34.0)\n",
      "Requirement already satisfied: socketIO-client-nexus>=0.7.6 in /usr/local/lib/python3.7/site-packages (from pyEX) (0.7.6)\n",
      "Requirement already satisfied: temporal-cache>=0.1.1 in /usr/local/lib/python3.7/site-packages (from pyEX) (0.1.4)\n",
      "Requirement already satisfied: sseclient>=0.0.22 in /usr/local/lib/python3.7/site-packages (from pyEX) (0.0.27)\n",
      "Requirement already satisfied: pytz>=2019.1 in /usr/local/lib/python3.7/site-packages (from pyEX) (2022.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from deprecation>=2.0.6->pyEX) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas>=0.22->pyEX) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib64/python3.7/site-packages (from pandas>=0.22->pyEX) (1.21.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.21.0->pyEX) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/site-packages (from requests>=2.21.0->pyEX) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.21.0->pyEX) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.21.0->pyEX) (3.4)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (0.18.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (5.6.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (2.13.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./virtualenv_spark-5e504eae930646b390f566675dd38d3c_0/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (28.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (3.0.33)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (0.1.6)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/site-packages (from ipython>=7.2.0->pyEX) (5.1.1)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/site-packages (from socketIO-client-nexus>=0.7.6->pyEX) (1.4.2)\n",
      "Requirement already satisfied: tzlocal>=2.0.0 in /usr/local/lib/python3.7/site-packages (from temporal-cache>=0.1.1->pyEX) (4.2)\n",
      "Requirement already satisfied: frozendict>=1.2 in /usr/local/lib64/python3.7/site-packages (from temporal-cache>=0.1.1->pyEX) (2.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->deprecation>=2.0.6->pyEX) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.2.0->pyEX) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.2.0->pyEX) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.2.0->pyEX) (0.7.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.7/site-packages (from tzlocal>=2.0.0->temporal-cache>=0.1.1->pyEX) (0.1.0.post0)\n",
      "Requirement already satisfied: backports.zoneinfo; python_version < \"3.9\" in /usr/local/lib64/python3.7/site-packages (from tzlocal>=2.0.0->temporal-cache>=0.1.1->pyEX) (0.2.1)\n",
      "Requirement already satisfied: tzdata; python_version >= \"3.6\" in /usr/local/lib/python3.7/site-packages (from pytz-deprecation-shim->tzlocal>=2.0.0->temporal-cache>=0.1.1->pyEX) (2022.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1a13f6-a7a6-4999-bffd-0a4190e5ba02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:34:44.785344Z",
     "iopub.status.busy": "2023-01-17T00:34:44.785004Z",
     "iopub.status.idle": "2023-01-17T00:34:47.810581Z",
     "shell.execute_reply": "2023-01-17T00:34:47.809686Z",
     "shell.execute_reply.started": "2023-01-17T00:34:44.785314Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awswrangler in /usr/local/lib/python3.7/site-packages (2.19.0)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.27.11 in /usr/local/lib/python3.7/site-packages (from awswrangler) (1.29.50)\n",
      "Requirement already satisfied: gremlinpython<4.0.0,>=3.5.2 in /usr/local/lib/python3.7/site-packages (from awswrangler) (3.6.1)\n",
      "Requirement already satisfied: numpy<=1.23.4,>=1.21.0; python_full_version >= \"3.7.1\" and python_full_version < \"3.11.0\" in /usr/local/lib64/python3.7/site-packages (from awswrangler) (1.21.6)\n",
      "Requirement already satisfied: redshift-connector<2.1.0,>=2.0.889 in /usr/local/lib64/python3.7/site-packages (from awswrangler) (2.0.909)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.11.1 in /usr/local/lib/python3.7/site-packages (from awswrangler) (2.2.1)\n",
      "Requirement already satisfied: requests-aws4auth<2.0.0,>=1.1.1 in /usr/local/lib/python3.7/site-packages (from awswrangler) (1.1.2)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.24.11 in /usr/local/lib/python3.7/site-packages (from awswrangler) (1.26.50)\n",
      "Requirement already satisfied: pyarrow<10.1.0,>=2.0.0 in /usr/local/lib64/python3.7/site-packages (from awswrangler) (10.0.1)\n",
      "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.5.3 in /usr/local/lib/python3.7/site-packages (from awswrangler) (1.5.3)\n",
      "Requirement already satisfied: openpyxl<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/site-packages (from awswrangler) (3.0.10)\n",
      "Requirement already satisfied: opensearch-py<3,>=1 in /usr/local/lib/python3.7/site-packages (from awswrangler) (2.0.1)\n",
      "Requirement already satisfied: pg8000<2.0.0,>=1.20.0 in /usr/local/lib/python3.7/site-packages (from awswrangler) (1.29.4)\n",
      "Requirement already satisfied: progressbar2<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/site-packages (from awswrangler) (4.2.0)\n",
      "Requirement already satisfied: pymysql<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from awswrangler) (1.0.2)\n",
      "Requirement already satisfied: pandas!=1.5.0,<2.0.0,<=1.5.1,>=1.2.0 in /usr/local/lib64/python3.7/site-packages (from awswrangler) (1.3.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore<2.0.0,>=1.27.11->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/site-packages (from botocore<2.0.0,>=1.27.11->awswrangler) (1.26.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<2.0.0,>=1.27.11->awswrangler) (2.8.2)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (0.6.1)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.5.6)\n",
      "Collecting aiohttp<=3.8.1,>=3.8.0\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aenum<4.0.0,>=1.4.5 in /usr/local/lib/python3.7/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (3.1.11)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (2022.6)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /usr/local/lib64/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (4.9.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /usr/local/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (4.11.1)\n",
      "Requirement already satisfied: scramp<1.5.0,>=1.2.0 in /usr/local/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (1.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (21.3)\n",
      "Requirement already satisfied: setuptools in ./virtualenv_spark-5e504eae930646b390f566675dd38d3c_0/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (28.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from requests-aws4auth<2.0.0,>=1.1.1->awswrangler) (1.16.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from boto3<2.0.0,>=1.24.11->awswrangler) (0.6.0)\n",
      "Requirement already satisfied: ply in /usr/local/lib/python3.7/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (3.11)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (5.1.1)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/site-packages (from openpyxl<3.1.0,>=3.0.0->awswrangler) (1.1.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/site-packages (from opensearch-py<3,>=1->awswrangler) (2022.9.24)\n",
      "Requirement already satisfied: importlib-metadata>=1.0; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from pg8000<2.0.0,>=1.20.0->awswrangler) (5.1.0)\n",
      "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.7/site-packages (from progressbar2<5.0.0,>=4.0.0->awswrangler) (3.4.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib64/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib64/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib64/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (0.13.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.3.2.post1)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in /usr/local/lib/python3.7/site-packages (from scramp<1.5.0,>=1.2.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (1.5.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->redshift-connector<2.1.0,>=2.0.889->awswrangler) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.0; python_version < \"3.8\"->pg8000<2.0.0,>=1.20.0->awswrangler) (3.11.0)\n",
      "Installing collected packages: aiohttp\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "aiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you'll have botocore 1.29.50 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27b0e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:34:47.812648Z",
     "iopub.status.busy": "2023-01-17T00:34:47.812299Z",
     "iopub.status.idle": "2023-01-17T00:34:51.375308Z",
     "shell.execute_reply": "2023-01-17T00:34:51.374402Z",
     "shell.execute_reply.started": "2023-01-17T00:34:47.812617Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tzlocal/unix.py:177: UserWarning: Can not find any timezone configuration, defaulting to UTC.\n",
      "  warnings.warn(\"Can not find any timezone configuration, defaulting to UTC.\")\n"
     ]
    }
   ],
   "source": [
    "import pyEX as p\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import vectorbt as vbt\n",
    "import matplotlib.pyplot as plt \n",
    "import itertools \n",
    "import datetime \n",
    "import awswrangler as wr\n",
    "import vectorbt as vbt\n",
    "import pyspark.sql.functions as py_f\n",
    "import pyspark.sql.types as py_t\n",
    "isSandbox = False\n",
    "if isSandbox:\n",
    "    token ='Tpk_02dcd2036e7641b880dd4cbb01fa9c67'\n",
    "    iex_ver = 'sandbox'\n",
    "else:\n",
    "    token ='pk_2e94555e43da4135a6032433c6b18fa5'\n",
    "    iex_ver = 'stable'\n",
    "pyEX_cl = p.Client(api_token=token)\n",
    "timeframe='max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92276f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:34:51.376949Z",
     "iopub.status.busy": "2023-01-17T00:34:51.376463Z",
     "iopub.status.idle": "2023-01-17T00:34:51.385562Z",
     "shell.execute_reply": "2023-01-17T00:34:51.384814Z",
     "shell.execute_reply.started": "2023-01-17T00:34:51.376921Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    fx_etf_symbols = ['FXA','FXY','FXE','FXB','UUP','FXC','FXF',]\n",
    "    bitcoin_etf_symbols = ['BITO']\n",
    "    commod_etf_symbols = ['USO','GLD','DBA','DBB','SLV',]\n",
    "    fi_etf_symbols = ['IAGG','AGG','IHY','EMLC','HYG',]\n",
    "    equity_etf_symbols = ['AMJ','AMLP','ARKF','ARKG','ARKK','ARKQ','ARKW','BOTZ','BUG','CGW','CIBR','COPX','CRBN','DRIV','EFV','EMLP','FBT','FDN','FIVG',\n",
    "    'FIW','FTXG','GDX','GNR','GUNR','HACK','IAT','IBB','ICLN','IEO','IFRA','IGE','IGF','IGM','IGV','IHF','IHI','ITB','ITZ','IWD','IWN','IWS','IYT','JETS','KBWB',\n",
    "                          'KOMP','KRE','KWEB','LCTU','LIT','MLPA',\n",
    "    'MLPX','MOO','NFRA','OIH','PABU','PAVE','PBW','PEJ','PHO','PICK','PPA','QCLN','REET','REMX','ROBO','SCHH','SIL','SKYY','SMH','SOXX','SRVR','TAN','URA','URNM',\n",
    "                          'VUG','XAR','XBI','XHB','XME','XOP','XSD','QQQ','EWZ','AAXJ','VTI','EWJ','EWA','EWC','MCHI','EWU','EWQ','EWG','FM','EIS']\n",
    "    sectors = ['XLC','XLY','XLP','XLE','XLF','XLV','XLI','XLB','XLRE','XLK','XLU']\n",
    "    bench_symbol=['SPY','QQQ','IVV']\n",
    "    all_symbols =fx_etf_symbols+bitcoin_etf_symbols+commod_etf_symbols+fi_etf_symbols+equity_etf_symbols+bench_symbol+sectors\n",
    "    #all_symbols\n",
    "    columns_to_capture = ['close','open','symbol']\n",
    "    data_folder = 's3://fsidatalake/eod_market_data/etf_trend_following/'\n",
    "    parquet_file_out =f\"{data_folder}/etf_market_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6876cc4f-8958-462f-952c-ff9dc7da6c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:34:51.387373Z",
     "iopub.status.busy": "2023-01-17T00:34:51.387062Z",
     "iopub.status.idle": "2023-01-17T00:34:51.395221Z",
     "shell.execute_reply": "2023-01-17T00:34:51.394533Z",
     "shell.execute_reply.started": "2023-01-17T00:34:51.387348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_full_file_name(data_folder,one_sym):\n",
    "    return(f\"{data_folder}{one_sym}.parquet\")\n",
    "def combine_new_and_stored_data(new_df,file_name,columns_to_capture):\n",
    "    try:\n",
    "        old_pd = pd.read_parquet(file_name)\n",
    "        final_df=pd.concat([old_pd,new_df[columns_to_capture]]).drop_duplicates().sort_index()\n",
    "    except Exception as l_exc:\n",
    "        final_df=new_df.drop_duplicates().sort_index()\n",
    "    return(final_df)\n",
    "if False:\n",
    "    md_df = None\n",
    "    for one_sym in all_symbols:\n",
    "        try:\n",
    "            new_df = pyEX_cl.chartDF(symbol=one_sym, timeframe=timeframe)\n",
    "            file_name = get_full_file_name(data_folder,one_sym)\n",
    "            final_df = combine_new_and_stored_data(new_df,file_name,columns_to_capture)\n",
    "            assert final_df['symbol'].values[0]==one_sym, 'STOP!!!'\n",
    "            assert one_sym in file_name, 'STOP!!!'\n",
    "            final_df[columns_to_capture].to_parquet(file_name,index=True)\n",
    "            if md_df is None:\n",
    "                md_df= spark.createDataFrame(final_df[columns_to_capture].reset_index())\n",
    "            else:\n",
    "                md_df=md_df.union(spark.createDataFrame(final_df[columns_to_capture].reset_index()))\n",
    "        except Exception as l_exc:\n",
    "            print(f\"exc:{one_sym}, {l_exc}\")\n",
    "        #break\n",
    "    md_df.write\\\n",
    "    .option(\"header\",True)\\\n",
    "    .partitionBy('symbol')\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(parquet_file_out)\n",
    "    print(new_df.index.min(),new_df.index.max(),final_df.index.min(),final_df.index.max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fd238c-883a-40ea-887b-9c32196e8779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T00:34:52.653522Z",
     "iopub.status.busy": "2023-01-17T00:34:52.653096Z",
     "iopub.status.idle": "2023-01-17T00:34:52.877146Z",
     "shell.execute_reply": "2023-01-17T00:34:52.875996Z",
     "shell.execute_reply.started": "2023-01-17T00:34:52.653492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f7b548c2-0945-4b37-9a45-660c885d2663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T04:49:21.373805Z",
     "iopub.status.busy": "2023-01-17T04:49:21.373355Z",
     "iopub.status.idle": "2023-01-17T04:49:21.584757Z",
     "shell.execute_reply": "2023-01-17T04:49:21.584019Z",
     "shell.execute_reply.started": "2023-01-17T04:49:21.373775Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panda UDF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>max_date</th>\n",
       "      <th>min_date</th>\n",
       "      <th>date_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BITS</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>423 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol   max_date   min_date date_length\n",
       "95   BITS 2023-01-13 2021-11-16    423 days"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_pd,max_date,hwm_l,local_sym,sym_pd=get_new_data_df(pd.DataFrame(['BITS'],columns=['symbol']))\n",
    "sym_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "eeed7ed0-7469-4eff-adf8-a977554f97ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T05:43:40.164754Z",
     "iopub.status.busy": "2023-01-17T05:43:40.163955Z",
     "iopub.status.idle": "2023-01-17T05:44:24.992023Z",
     "shell.execute_reply": "2023-01-17T05:44:24.991128Z",
     "shell.execute_reply.started": "2023-01-17T05:43:40.164718Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [symbol, data_count]\n",
      "Index: []\n",
      "no dups\n"
     ]
    }
   ],
   "source": [
    "#@py_f.pandas_udf(\"date timestamp, close double, open double, symbol string\", py_f.PandasUDFType.GROUPED_MAP)\n",
    "#@py_f.pandas_udf(\"symbol string\", py_f.PandasUDFType.GROUPED_MAP)\n",
    "def get_new_data_df(symbol):\n",
    "    local_sym=str(symbol.values[0][0])\n",
    "    try:\n",
    "        hwm_l=highwatermark_pd_bc.value\n",
    "        columns_to_capture=columns_to_capture_bc.value\n",
    "        iex_base_url=iex_base_url_bc.value\n",
    "        sym_pd = hwm_l.query(f\"symbol=='{local_sym}'\")\n",
    "        if len(sym_pd)>0:\n",
    "            max_date = sym_pd['max_date'].iloc[0]\n",
    "        else:\n",
    "            max_date = datetime.datetime.now()-datetime.timedelta(days=365*50)\n",
    "        last_days=(datetime.datetime.now()-max_date).days\n",
    "\n",
    "        pyEX_cl = p.Client(api_token=iex_token_bc.value)\n",
    "        final_url = iex_base_url.format(local_sym,iex_token_bc.value,int(last_days))\n",
    "        #'https://cloud.iexapis.com/stable/stock/{}/chart/max?token={}&chartIEXOnly=true&chartLast={}'\n",
    "        res = requests.get(final_url,headers=headers)\n",
    "        ret_pd = pd.DataFrame(json.loads(res.text))#[columns_to_capture]\n",
    "        ret_pd=ret_pd.query(f\"date>'{max_date.strftime('%Y-%m-%d')}'\")\n",
    "        ret_pd['date']=pd.to_datetime(ret_pd['date'])\n",
    "        print('panda UDF')\n",
    "        #return(ret_pd[columns_to_capture].query(f\"date>'{max_date}'\"))\n",
    "    except:\n",
    "        ret_pd=pd.DataFrame([{'date': datetime.datetime.now()-datetime.timedelta(days=365*50),\n",
    "                              'close': None, \n",
    "                              'open': None, \n",
    "                              'symbol': local_sym}])\n",
    "    #return(ret_pd[columns_to_capture],max_date,hwm_l,local_sym,sym_pd)\n",
    "    return(ret_pd[columns_to_capture])\n",
    "    #return(pd.DataFrame([res.text],columns=['symbol']))\n",
    "    \n",
    "class MarketDataGateway():\n",
    "    def __init__(self,isSandbox=False):\n",
    "        self.fx_etf_symbols = ['FXA','FXY','FXE','FXB','UUP','FXC','FXF',]\n",
    "        self.bitcoin_etf_symbols = ['BITO','BTF','XBTF','BITS','GBTC','BITW','BLOK','BLCN','LEGR','SPBC','BITQ','BKCH','DAPP','BTCFX']\n",
    "        self.bitcoin_equity_proxy_symbols = ['COIN','MSTR']\n",
    "        self.crypto_mining_symbols = ['RIOT','CAN','HUT','HIVE','MARA','BTCM','BTBT','BITF']\n",
    "        self.commod_etf_symbols = ['USO','GLD','DBA','DBB','SLV',]\n",
    "        self.fi_etf_symbols = ['IAGG','AGG','IHY','EMLC','HYG',]\n",
    "        self.equity_etf_symbols = ['AMJ','AMLP','ARKF','ARKG','ARKK','ARKQ','ARKW','BOTZ','BUG','CGW','CIBR','COPX','CRBN','DRIV','EFV','EMLP','FBT','FDN','FIVG',\n",
    "                'FIW','FTXG','GDX','GNR','GUNR','HACK','IAT','IBB','ICLN','IEO','IFRA','IGE','IGF','IGM','IGV','IHF','IHI','ITB','ITZ','IWD','IWN','IWS','IYT','JETS','KBWB',\n",
    "                      'KOMP','KRE','KWEB','LCTU','LIT','MLPA',\n",
    "                    'MLPX','MOO','NFRA','OIH','PABU','PAVE','PBW','PEJ','PHO','PICK','PPA','QCLN','REET','REMX','ROBO','SCHH','SIL','SKYY','SMH','SOXX','SRVR','TAN','URA','URNM',\n",
    "                      'VUG','XAR','XBI','XHB','XME','XOP','XSD','QQQ','EWZ','AAXJ','VTI','EWJ','EWA','EWC','MCHI','EWU','EWQ','EWG','FM','EIS']\n",
    "        self.sectors = ['XLC','XLY','XLP','XLE','XLF','XLV','XLI','XLB','XLRE','XLK','XLU']\n",
    "        self.bench_symbol=['SPY','QQQ','IVV']\n",
    "        self.all_symbols =self.fx_etf_symbols+\\\n",
    "                            self.bitcoin_etf_symbols+\\\n",
    "                            self.commod_etf_symbols+\\\n",
    "                            self.bitcoin_equity_proxy_symbols+\\\n",
    "                            self.crypto_mining_symbols+\\\n",
    "                            self.fi_etf_symbols+\\\n",
    "                            self.equity_etf_symbols+\\\n",
    "                            self.bench_symbol+\\\n",
    "                            self.sectors\n",
    "        self.all_symbols_df = spark.createDataFrame(pd.DataFrame(self.all_symbols,columns=['symbol']))\n",
    "        #all_symbols\n",
    "        self.columns_to_capture = ['date','close','open','symbol']\n",
    "        self.data_folder = 's3://fsidatalake/eod_market_data/etf_trend_following/'\n",
    "        self.parquet_file_out =f\"{self.data_folder}/etf_market_data.parquet\"\n",
    "        if isSandbox:\n",
    "            self.iex_token ='Tpk_02dcd2036e7641b880dd4cbb01fa9c67'\n",
    "            self.iex_ver = 'sandbox'\n",
    "        else:\n",
    "            self.iex_token ='pk_2e94555e43da4135a6032433c6b18fa5'\n",
    "            self.iex_ver = 'stable'\n",
    "        self.iex_base_url = 'https://cloud.iexapis.com/'+self.iex_ver+'/stock/{}/chart/max?token={}&chartIEXOnly=true&chartLast={}'\n",
    "        \n",
    "    def set_highwatermarks(self):\n",
    "        try:\n",
    "            self.old_df = spark.read.parquet(self.parquet_file_out)\n",
    "        except:\n",
    "            self.old_df = spark.createDataFrame(pd.DataFrame([{'symbol':'----','date':datetime.datetime.now()-datetime.timedelta(days=365*50)}]))\n",
    "        self.highwatermark_pd = self.old_df.groupby('symbol').agg(py_f.max(\"date\").alias(\"max_date\"),\n",
    "                             py_f.min(\"date\").alias(\"min_date\"),\n",
    "                             (py_f.max(\"date\")-py_f.min(\"date\")).alias(\"date_length\")\n",
    "                            ).toPandas()\n",
    "    def set_new_symbol_df(self):\n",
    "        self.new_sym_df=self.all_symbols_df.groupby(\"symbol\")\\\n",
    "                        .applyInPandas(get_new_data_df, schema=\"date timestamp, close double, open double, symbol string\")\\\n",
    "                        .where('close is not null')\n",
    "    def write_new_symbol_df(self):\n",
    "        self.write_stats=self.new_sym_df.groupby('symbol').agg(py_f.count('symbol').alias('data_count'))\n",
    "        self.new_sym_df.write\\\n",
    "            .option(\"header\",True)\\\n",
    "            .partitionBy('symbol')\\\n",
    "            .mode(\"append\")\\\n",
    "            .parquet(self.parquet_file_out)\n",
    "    def rewrite_if_dups(self):\n",
    "        self.old_df_after_write = spark.read.parquet(self.parquet_file_out)\n",
    "        self.dup_df = self.old_df_after_write.distinct().groupby('date','symbol').count()\n",
    "        dup_count=self.dup_df.where('count>1').count()\n",
    "        if dup_count>0:        \n",
    "            print(f\"found:{dup_count} dups; sample:{self.dup_df.show()}\")\n",
    "            self.old_df_after_write.groupby('date','symbol').agg(py_f.mean('close').alias('close'),py_f.mean('open').alias('open'))\\\n",
    "            .write\\\n",
    "            .option(\"header\",True)\\\n",
    "            .partitionBy('symbol')\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .parquet(self.parquet_file_out.replace('.parquet','_temp.parquet'))\n",
    "        else:\n",
    "            print(f\"no dups\")\n",
    "        \n",
    "\n",
    "mdg=MarketDataGateway()\n",
    "mdg.set_highwatermarks()\n",
    "# set bc vars\n",
    "highwatermark_pd_bc =sc.broadcast(mdg.highwatermark_pd)\n",
    "columns_to_capture_bc =sc.broadcast(mdg.columns_to_capture)\n",
    "iex_token_bc =sc.broadcast(mdg.iex_token)\n",
    "iex_base_url_bc =sc.broadcast(mdg.iex_base_url)\n",
    "# set bc vars\n",
    "mdg.set_new_symbol_df()\n",
    "mdg.write_new_symbol_df()\n",
    "print(mdg.write_stats.toPandas().sort_values(['symbol']))\n",
    "mdg.rewrite_if_dups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "bfecb398-68e1-4367-8a4f-e82486f4ed3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T05:05:25.135891Z",
     "iopub.status.busy": "2023-01-17T05:05:25.135438Z",
     "iopub.status.idle": "2023-01-17T05:05:26.064057Z",
     "shell.execute_reply": "2023-01-17T05:05:26.063226Z",
     "shell.execute_reply.started": "2023-01-17T05:05:25.135861Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5195"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdg.old_df_after_write.distinct().groupby('date','symbol').count().where('count>1').count()#.orderBy(['count','symbol'],ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9711086d-f0e5-44bd-b32b-548816b9c6d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T05:32:11.869742Z",
     "iopub.status.busy": "2023-01-17T05:32:11.869321Z",
     "iopub.status.idle": "2023-01-17T05:32:27.227185Z",
     "shell.execute_reply": "2023-01-17T05:32:27.225738Z",
     "shell.execute_reply.started": "2023-01-17T05:32:11.869711Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----+\n",
      "|               date|symbol|count|\n",
      "+-------------------+------+-----+\n",
      "|2008-09-17 00:00:00|   EWZ|    1|\n",
      "|2020-11-27 00:00:00|   PHO|    1|\n",
      "|2021-11-01 00:00:00|   IGV|    1|\n",
      "|2007-05-23 00:00:00|   VTI|    1|\n",
      "|2022-02-15 00:00:00|  KOMP|    1|\n",
      "|2021-05-04 00:00:00|  SRVR|    1|\n",
      "|2018-12-31 00:00:00|   XHB|    1|\n",
      "|2016-05-19 00:00:00|  HACK|    1|\n",
      "|2015-04-08 00:00:00|  RIOT|    1|\n",
      "|2020-11-20 00:00:00|  IFRA|    1|\n",
      "|2022-06-07 00:00:00|  IAGG|    1|\n",
      "|2018-06-12 00:00:00|  ARKW|    1|\n",
      "|2018-10-19 00:00:00|  AAXJ|    1|\n",
      "|2013-10-16 00:00:00|   SPY|    1|\n",
      "|2011-07-06 00:00:00|   SPY|    1|\n",
      "|2018-08-01 00:00:00|  MLPX|    1|\n",
      "|2011-06-08 00:00:00|   FDN|    1|\n",
      "|2020-03-06 00:00:00|   EWC|    1|\n",
      "|2013-04-17 00:00:00|   SIL|    1|\n",
      "|2016-07-14 00:00:00|   EWC|    1|\n",
      "+-------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "found:417044 dups; sample:None\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o9065.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat sun.reflect.GeneratedMethodAccessor549.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1099.0 failed 4 times, most recent failure: Lost task 4.3 in stage 1099.0 (TID 26093) (10.0.152.112 executor 279): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://fsidatalake/eod_market_data/etf_trend_following/etf_market_data.parquet/symbol=XLP/part-00000-ff4e8a07-36d8-41dd-9e8c-15a647946f17.c000.snappy.parquet, range: 0-57427, partition values: [XLP], isDataPresent: false, eTag: 52c9370656b9655dd0e9f9d6f9edb27c-1\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tSuppressed: java.io.FileNotFoundException: File not present on S3\n\t\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:273)\n\t\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\t\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\t\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\t\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\t\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t\t... 3 more\nCaused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.InterruptedException: sleep interrupted\n\tat java.lang.Thread.sleep(Native Method)\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:491)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:251)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:123)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:251)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:139)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:251)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:232)\n\t... 47 more\nCaused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://fsidatalake/eod_market_data/etf_trend_following/etf_market_data.parquet/symbol=XLP/part-00000-ff4e8a07-36d8-41dd-9e8c-15a647946f17.c000.snappy.parquet, range: 0-57427, partition values: [XLP], isDataPresent: false, eTag: 52c9370656b9655dd0e9f9d6f9edb27c-1\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.io.FileNotFoundException: File not present on S3\n\t\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:273)\n\t\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\t\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\t\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\t\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\t\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t\t... 3 more\nCaused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.InterruptedException: sleep interrupted\n\tat java.lang.Thread.sleep(Native Method)\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewrite_if_dups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36mrewrite_if_dups\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'symbol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet_file_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o9065.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat sun.reflect.GeneratedMethodAccessor549.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1099.0 failed 4 times, most recent failure: Lost task 4.3 in stage 1099.0 (TID 26093) (10.0.152.112 executor 279): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://fsidatalake/eod_market_data/etf_trend_following/etf_market_data.parquet/symbol=XLP/part-00000-ff4e8a07-36d8-41dd-9e8c-15a647946f17.c000.snappy.parquet, range: 0-57427, partition values: [XLP], isDataPresent: false, eTag: 52c9370656b9655dd0e9f9d6f9edb27c-1\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tSuppressed: java.io.FileNotFoundException: File not present on S3\n\t\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:273)\n\t\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\t\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\t\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\t\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\t\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t\t... 3 more\nCaused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.InterruptedException: sleep interrupted\n\tat java.lang.Thread.sleep(Native Method)\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:237)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:505)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:491)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:251)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:123)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:251)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:139)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:251)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:232)\n\t... 47 more\nCaused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://fsidatalake/eod_market_data/etf_trend_following/etf_market_data.parquet/symbol=XLP/part-00000-ff4e8a07-36d8-41dd-9e8c-15a647946f17.c000.snappy.parquet, range: 0-57427, partition values: [XLP], isDataPresent: false, eTag: 52c9370656b9655dd0e9f9d6f9edb27c-1\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.io.FileNotFoundException: File not present on S3\n\t\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:273)\n\t\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\t\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\t\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\t\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\t\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\t\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t\t... 3 more\nCaused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.io.DelegatingSeekableInputStream.read(DelegatingSeekableInputStream.java:61)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:555)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:803)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.InterruptedException: sleep interrupted\n\tat java.lang.Thread.sleep(Native Method)\n\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "mdg.rewrite_if_dups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "fb2d50bd-5ddd-4cd4-a3fb-e6d73d1ea364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T04:28:17.403875Z",
     "iopub.status.busy": "2023-01-17T04:28:17.403563Z",
     "iopub.status.idle": "2023-01-17T04:28:17.444926Z",
     "shell.execute_reply": "2023-01-17T04:28:17.444067Z",
     "shell.execute_reply.started": "2023-01-17T04:28:17.403851Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>priceDate</th>\n",
       "      <th>symbol</th>\n",
       "      <th>volume</th>\n",
       "      <th>id</th>\n",
       "      <th>key</th>\n",
       "      <th>subkey</th>\n",
       "      <th>...</th>\n",
       "      <th>uLow</th>\n",
       "      <th>uVolume</th>\n",
       "      <th>fOpen</th>\n",
       "      <th>fClose</th>\n",
       "      <th>fHigh</th>\n",
       "      <th>fLow</th>\n",
       "      <th>fVolume</th>\n",
       "      <th>label</th>\n",
       "      <th>change</th>\n",
       "      <th>changePercent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-13</th>\n",
       "      <td>164.28</td>\n",
       "      <td>166.0100</td>\n",
       "      <td>164.1600</td>\n",
       "      <td>164.32</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1390667</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>164.1600</td>\n",
       "      <td>1390667</td>\n",
       "      <td>164.3200</td>\n",
       "      <td>164.2800</td>\n",
       "      <td>166.0100</td>\n",
       "      <td>164.1600</td>\n",
       "      <td>1390667</td>\n",
       "      <td>Jan 13, 23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-12</th>\n",
       "      <td>164.90</td>\n",
       "      <td>165.9650</td>\n",
       "      <td>164.1200</td>\n",
       "      <td>165.26</td>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1041570</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>164.1200</td>\n",
       "      <td>1041570</td>\n",
       "      <td>165.2600</td>\n",
       "      <td>164.9000</td>\n",
       "      <td>165.9650</td>\n",
       "      <td>164.1200</td>\n",
       "      <td>1041570</td>\n",
       "      <td>Jan 12, 23</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-11</th>\n",
       "      <td>165.39</td>\n",
       "      <td>168.8400</td>\n",
       "      <td>164.7600</td>\n",
       "      <td>167.70</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1225794</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>164.7600</td>\n",
       "      <td>1225794</td>\n",
       "      <td>167.7000</td>\n",
       "      <td>165.3900</td>\n",
       "      <td>168.8400</td>\n",
       "      <td>164.7600</td>\n",
       "      <td>1225794</td>\n",
       "      <td>Jan 11, 23</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10</th>\n",
       "      <td>167.26</td>\n",
       "      <td>167.9500</td>\n",
       "      <td>164.9000</td>\n",
       "      <td>165.82</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1126569</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>164.9000</td>\n",
       "      <td>1126569</td>\n",
       "      <td>165.8200</td>\n",
       "      <td>167.2600</td>\n",
       "      <td>167.9500</td>\n",
       "      <td>164.9000</td>\n",
       "      <td>1126569</td>\n",
       "      <td>Jan 10, 23</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.0113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-09</th>\n",
       "      <td>164.97</td>\n",
       "      <td>167.7200</td>\n",
       "      <td>164.9400</td>\n",
       "      <td>166.21</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1323620</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>164.9400</td>\n",
       "      <td>1323620</td>\n",
       "      <td>166.2100</td>\n",
       "      <td>164.9700</td>\n",
       "      <td>167.7200</td>\n",
       "      <td>164.9400</td>\n",
       "      <td>1323620</td>\n",
       "      <td>Jan 9, 23</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>-0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>46.45</td>\n",
       "      <td>46.6000</td>\n",
       "      <td>45.9000</td>\n",
       "      <td>46.21</td>\n",
       "      <td>2007-01-08</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1844400</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>45.9000</td>\n",
       "      <td>1844400</td>\n",
       "      <td>37.7994</td>\n",
       "      <td>37.9958</td>\n",
       "      <td>38.1185</td>\n",
       "      <td>37.5459</td>\n",
       "      <td>1844400</td>\n",
       "      <td>Jan 8, 07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>45.73</td>\n",
       "      <td>46.4034</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>46.11</td>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>ABC</td>\n",
       "      <td>2005300</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>45.5600</td>\n",
       "      <td>2005300</td>\n",
       "      <td>37.7176</td>\n",
       "      <td>37.4068</td>\n",
       "      <td>37.9576</td>\n",
       "      <td>37.2677</td>\n",
       "      <td>2005300</td>\n",
       "      <td>Jan 5, 07</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>46.29</td>\n",
       "      <td>46.4100</td>\n",
       "      <td>45.6501</td>\n",
       "      <td>46.23</td>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1390200</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>45.6501</td>\n",
       "      <td>1390200</td>\n",
       "      <td>37.8158</td>\n",
       "      <td>37.8649</td>\n",
       "      <td>37.9630</td>\n",
       "      <td>37.3414</td>\n",
       "      <td>1390200</td>\n",
       "      <td>Jan 4, 07</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>46.11</td>\n",
       "      <td>46.1500</td>\n",
       "      <td>45.0800</td>\n",
       "      <td>45.08</td>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>ABC</td>\n",
       "      <td>2735700</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>45.0800</td>\n",
       "      <td>2735700</td>\n",
       "      <td>36.8751</td>\n",
       "      <td>37.7176</td>\n",
       "      <td>37.7504</td>\n",
       "      <td>36.8751</td>\n",
       "      <td>2735700</td>\n",
       "      <td>Jan 3, 07</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.0039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-29</th>\n",
       "      <td>44.96</td>\n",
       "      <td>45.1400</td>\n",
       "      <td>44.9200</td>\n",
       "      <td>44.96</td>\n",
       "      <td>2006-12-29</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1198500</td>\n",
       "      <td>HISTORICAL_PRICES</td>\n",
       "      <td>ABC</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>44.9200</td>\n",
       "      <td>1198500</td>\n",
       "      <td>36.7770</td>\n",
       "      <td>36.7770</td>\n",
       "      <td>36.9242</td>\n",
       "      <td>36.7442</td>\n",
       "      <td>1198500</td>\n",
       "      <td>Dec 29, 06</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.0249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4045 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             close      high       low    open   priceDate symbol   volume  \\\n",
       "date                                                                         \n",
       "2023-01-13  164.28  166.0100  164.1600  164.32  2023-01-13    ABC  1390667   \n",
       "2023-01-12  164.90  165.9650  164.1200  165.26  2023-01-12    ABC  1041570   \n",
       "2023-01-11  165.39  168.8400  164.7600  167.70  2023-01-11    ABC  1225794   \n",
       "2023-01-10  167.26  167.9500  164.9000  165.82  2023-01-10    ABC  1126569   \n",
       "2023-01-09  164.97  167.7200  164.9400  166.21  2023-01-09    ABC  1323620   \n",
       "...            ...       ...       ...     ...         ...    ...      ...   \n",
       "2007-01-08   46.45   46.6000   45.9000   46.21  2007-01-08    ABC  1844400   \n",
       "2007-01-05   45.73   46.4034   45.5600   46.11  2007-01-05    ABC  2005300   \n",
       "2007-01-04   46.29   46.4100   45.6501   46.23  2007-01-04    ABC  1390200   \n",
       "2007-01-03   46.11   46.1500   45.0800   45.08  2007-01-03    ABC  2735700   \n",
       "2006-12-29   44.96   45.1400   44.9200   44.96  2006-12-29    ABC  1198500   \n",
       "\n",
       "                           id  key subkey  ...      uLow  uVolume     fOpen  \\\n",
       "date                                       ...                                \n",
       "2023-01-13  HISTORICAL_PRICES  ABC         ...  164.1600  1390667  164.3200   \n",
       "2023-01-12  HISTORICAL_PRICES  ABC         ...  164.1200  1041570  165.2600   \n",
       "2023-01-11  HISTORICAL_PRICES  ABC         ...  164.7600  1225794  167.7000   \n",
       "2023-01-10  HISTORICAL_PRICES  ABC         ...  164.9000  1126569  165.8200   \n",
       "2023-01-09  HISTORICAL_PRICES  ABC         ...  164.9400  1323620  166.2100   \n",
       "...                       ...  ...    ...  ...       ...      ...       ...   \n",
       "2007-01-08  HISTORICAL_PRICES  ABC         ...   45.9000  1844400   37.7994   \n",
       "2007-01-05  HISTORICAL_PRICES  ABC         ...   45.5600  2005300   37.7176   \n",
       "2007-01-04  HISTORICAL_PRICES  ABC         ...   45.6501  1390200   37.8158   \n",
       "2007-01-03  HISTORICAL_PRICES  ABC         ...   45.0800  2735700   36.8751   \n",
       "2006-12-29  HISTORICAL_PRICES  ABC         ...   44.9200  1198500   36.7770   \n",
       "\n",
       "              fClose     fHigh      fLow  fVolume       label  change  \\\n",
       "date                                                                    \n",
       "2023-01-13  164.2800  166.0100  164.1600  1390667  Jan 13, 23    0.00   \n",
       "2023-01-12  164.9000  165.9650  164.1200  1041570  Jan 12, 23    0.62   \n",
       "2023-01-11  165.3900  168.8400  164.7600  1225794  Jan 11, 23    0.49   \n",
       "2023-01-10  167.2600  167.9500  164.9000  1126569  Jan 10, 23    1.87   \n",
       "2023-01-09  164.9700  167.7200  164.9400  1323620   Jan 9, 23   -2.29   \n",
       "...              ...       ...       ...      ...         ...     ...   \n",
       "2007-01-08   37.9958   38.1185   37.5459  1844400   Jan 8, 07    0.15   \n",
       "2007-01-05   37.4068   37.9576   37.2677  2005300   Jan 5, 07   -0.72   \n",
       "2007-01-04   37.8649   37.9630   37.3414  1390200   Jan 4, 07    0.56   \n",
       "2007-01-03   37.7176   37.7504   36.8751  2735700   Jan 3, 07   -0.18   \n",
       "2006-12-29   36.7770   36.9242   36.7442  1198500  Dec 29, 06   -1.15   \n",
       "\n",
       "            changePercent  \n",
       "date                       \n",
       "2023-01-13         0.0000  \n",
       "2023-01-12         0.0038  \n",
       "2023-01-11         0.0030  \n",
       "2023-01-10         0.0113  \n",
       "2023-01-09        -0.0137  \n",
       "...                   ...  \n",
       "2007-01-08         0.0032  \n",
       "2007-01-05        -0.0155  \n",
       "2007-01-04         0.0122  \n",
       "2007-01-03        -0.0039  \n",
       "2006-12-29        -0.0249  \n",
       "\n",
       "[4045 rows x 26 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lpd=mdg.highwatermark_pd\n",
    "#sym='abc'\n",
    "#sym_pd = lpd.query(f\"symbol=='{sym}'\")['max_date']\n",
    "#if len(sym_pd)>0:\n",
    "#    max_date = lpd.query(f\"symbol=='{sym}'\")['max_date'].iloc[0]\n",
    "#else:\n",
    "#    max_date = datetime.datetime.now()-datetime.timedelta(days=365*50)\n",
    "#last_days=(datetime.datetime.now()-max_date).days\n",
    "#res_pd = pyEX_cl.chartDF(symbol=sym, timeframe=timeframe,last=last_days).query(f\"date>'{(max_date)}'\")\n",
    "res_pd.query(f\"date>'{max_date.strftime('%Y-%m-%d')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "45a5624c-5627-43ab-834c-0bad2aba28d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T04:23:32.151768Z",
     "iopub.status.busy": "2023-01-17T04:23:32.151302Z",
     "iopub.status.idle": "2023-01-17T04:23:32.157193Z",
     "shell.execute_reply": "2023-01-17T04:23:32.156290Z",
     "shell.execute_reply.started": "2023-01-17T04:23:32.151739Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1973, 1, 29, 4, 22, 11, 48817)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9c2c739a-81cf-4989-9263-7af0f517695a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-17T04:30:30.915787Z",
     "iopub.status.busy": "2023-01-17T04:30:30.915329Z",
     "iopub.status.idle": "2023-01-17T04:30:31.064075Z",
     "shell.execute_reply": "2023-01-17T04:30:31.063092Z",
     "shell.execute_reply.started": "2023-01-17T04:30:30.915758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "#base_url = 'https://cloud.iexapis.com/stable/stock/{}/chart/max?token={}&chartIEXOnly=true&chartLast={}'\n",
    "#headers = {\n",
    "#      'content-type': \"application/json\"\n",
    "#  }\n",
    "\n",
    "res = requests.get(base_url)\n",
    "pd.DataFrame(json.loads(res.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24520251-97d6-492a-9fac-9475504d47c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T20:33:42.761799Z",
     "iopub.status.busy": "2023-01-16T20:33:42.761364Z",
     "iopub.status.idle": "2023-01-16T20:33:42.780884Z",
     "shell.execute_reply": "2023-01-16T20:33:42.779778Z",
     "shell.execute_reply.started": "2023-01-16T20:33:42.761766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mmax_date\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/pandas/_libs/tslibs/timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__sub__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/pandas/_libs/tslibs/timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__add__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`"
     ]
    }
   ],
   "source": [
    "(max_date-10).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d4ea8d3-4797-42e5-9991-c116cb05b642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T20:27:11.410442Z",
     "iopub.status.busy": "2023-01-16T20:27:11.409987Z",
     "iopub.status.idle": "2023-01-16T20:27:11.423811Z",
     "shell.execute_reply": "2023-01-16T20:27:11.422977Z",
     "shell.execute_reply.started": "2023-01-16T20:27:11.410412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>max_date</th>\n",
       "      <th>min_date</th>\n",
       "      <th>date_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IWN</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2006-12-29</td>\n",
       "      <td>5859 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EIS</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2008-05-29</td>\n",
       "      <td>5342 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMLC</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2010-07-23</td>\n",
       "      <td>4557 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XSD</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>5797 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REMX</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>4460 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>URNM</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>1136 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>KOMP</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>1543 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>XLC</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>1669 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>FIVG</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>1410 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ARKF</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>1439 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol   max_date   min_date date_length\n",
       "0      IWN 2023-01-13 2006-12-29   5859 days\n",
       "1      EIS 2023-01-13 2008-05-29   5342 days\n",
       "2     EMLC 2023-01-13 2010-07-23   4557 days\n",
       "3      XSD 2023-01-13 2007-03-01   5797 days\n",
       "4     REMX 2023-01-13 2010-10-28   4460 days\n",
       "..     ...        ...        ...         ...\n",
       "119   URNM 2023-01-13 2019-12-04   1136 days\n",
       "120   KOMP 2023-01-13 2018-10-23   1543 days\n",
       "121    XLC 2023-01-13 2018-06-19   1669 days\n",
       "122   FIVG 2023-01-13 2019-03-05   1410 days\n",
       "123   ARKF 2023-01-13 2019-02-04   1439 days\n",
       "\n",
       "[124 rows x 4 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad23ba-bfa8-4a5e-af25-184365c848c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
